{"config":{"lang":["en"],"separator":"[\\s\\-]+","pipeline":["stopWordFilter"]},"docs":[{"location":"","title":"Home","text":""},{"location":"#predicting-colitis-using-penalised-regression","title":"Predicting colitis using penalised regression","text":""},{"location":"#description","title":"Description","text":"<p>The field of predictive medicine is becoming increasingly popular. A key challenge is dealing with the high-dimensionality of genetics, where the number of genes (factors) is far larger than the number of patients (observations), resulting in classical statistical methods breaking down. This has lead to the rise of penalised regression, where a penalty is applied to the factors to induce sparsity, so that a large majority of factors are deemed irrelevant, allowing for statistical inference to occur. One popular penalised approach, covered in most undergraduate and graduate statistics modules, is the lasso. The lasso has gained a large amount of traction over the last 20 years, with many extensions also proposed. </p> <p>The lasso has found particular use in genetics, as it is computationally efficient and is able to select relevant genes as being associated with a disease. One particular useful extension is the group lasso, which can apply this penalisation onto groups of variables. As genes are naturally found in groups (pathways), this extension has also found extensive use in genetics. One final approach is the sparse-group lasso, which combines the two. This project shows how to apply these three methods to predicting whether a patient has colitis, an inflammatory bowel disease. </p>"},{"location":"#learning-outcomes","title":"Learning Outcomes","text":"<p>Main outcomes:</p> <ul> <li>Go through an end-to-end analysis of how we can use penalised regression models to predict cases of colitis using gene expression data.</li> <li>Gain an insight into how genetics data can be downloaded, cleaned, and prepared for use in analysis \u2013 this gives a good insight into how general R data manipulation works.</li> <li>Follow the introduction into predictive modelling by showing how a fitted model can be used to form predictions.</li> <li>Understand how the wide class of penalised regression models work and how they can be implemented in R. This will include mathematical and statistical background to how these methods work, which will touch upon important regression topics that form the foundation for most models used widely in academia and industry.</li> </ul> <p>Optional outcomes:</p> <ul> <li>The example provided is for predicting colitis data. The questions provided in each section will provide guidance through another example, where breast cancer is predicted. Therefore, the questions will form a comprehensive additional analysis of breast cancer data.</li> <li>An additional class of models, SLOPE models, are presented in the optional sections of the chapters. SLOPE models are adaptive versions of the models covered in the main outcomes and provide additional insight into the direction of the current research in this area.</li> </ul> Task Time Chapter 1 2 hours Chapter 2 5 hours Chapter 3 4 hours"},{"location":"#requirements","title":"Requirements","text":""},{"location":"#academic","title":"Academic","text":"<ul> <li>Basic understanding of mathematical concepts that underpin penalised regression: linear algebra, matrices, geometry.</li> <li>Basic statistics knowledge, including linear regression and model fitting.</li> <li>Genetics background is not needed. Basic background information is provided in Chapter 1.</li> <li>Familiarity with R programming language.</li> </ul>"},{"location":"#system","title":"System","text":"Program Version R Any"},{"location":"#getting-started","title":"Getting Started","text":"<p>The chapters are structured to be worked through sequentially. Each chapter contains optional content that adds extra insight into the problem but is not required to solve to the core problem. </p>"},{"location":"#license","title":"License","text":"<p>This project is licensed under the BSD-3-Clause license</p>"},{"location":"1-chapter-1/","title":"Chapter 1: Downloading genetics data","text":"<p>This chapter will give an overview of genetics data. We will cover:</p> <ol> <li>Genetics background (optional). In this section a brief background of genetics will be given, to give context to the problem, but it is optional, as relevant domain knowledge will also be inserted throughout the other chapters.</li> <li>Downloading genetics data. In this section, code is presented to readily download genetics data.</li> <li>Processing the data. Step-by-step instructions are given on how the data processing pipeline works. </li> </ol> <p>The aim of this chapter is to provide useful code that can be easily adapted and expanded upon to download different genetics datasets. In our processing section, we are interested in three objects:</p> <ol> <li>The input data matrix, . This is the data used to form inferences about the genes and will be gene expression data.</li> <li>The response vector, . This will contain the disease state of a patient (1 if the patient has the disease and 0 otherwise, or TRUE/FALSE).</li> <li>The grouping structure indexes. This will contain group indexes for which groups (pathways) the genes belong to. This is only needed for the models which use grouping information (glasso and SGL).</li> </ol> <p>Some questions to consider when reading this chapter are:</p> <ul> <li>Why do we need the various processing steps? Are these steps needed each time or can we sometimes skip some of them?</li> <li>Are there are steps that have been omitted that may be important?</li> <li>Is this pipeline applicable beyond genetics data?</li> </ul>"},{"location":"1-chapter-1/#genetics-background-optional","title":"Genetics background (optional)","text":"<p>There are 46 chromosomes within the nucleus of a human cell. Two copies of the 22 autosome chromosomes and the sex chromosomes XX and XY make up these 46. Each chromosome is made up of a deoxyribonucleic acid (DNA) molecule. The DNA is a double-stranded molecule which encodes genetic information about an individual. The figure below shows a visualisation of these concepts.</p> <p></p> <p>DNA is made up of bonds between base pairs of nucleotides. There are four types of nucleotides found in DNA: adenine (A), cytosine (C), guanine (G), and thymine (T); shown in the figure below.</p> <p></p> <p>Genes are regions of the DNA which act as instructions to make proteins. They also provide individuals with inherited characteristics. The section with letters in the figure above shows an example gene.</p> <p>The bonds between the nucleotides hold the DNA strand together in the form of a double helix. Single-nucleotide polymorphisms (SNPs) are common variations of the DNA observed in individuals; shown in the figure below. They are variations of single nucleotides of the genome. </p> <p>A locus is a fixed position on a chromosome which contains a gene [R1]. A DNA sequence at a locus can have different forms (called alleles) from one copy of the chromosome to another. An individual's genotype is their combination of alleles at a specific locus. </p> <p></p> <p>Let us consider the SNP shown in figure above with two alleles: A and G. Hence, an individual has four possible genotypes that can be observed: AA, AG, GA, and GG. The genotypes AA and GG are referred to as the homozygous genotypes. AG and GA are the heterozygous genotypes. The allele which is observed the least in a sample population is termed the minor allele. </p> <p>Finally, a phenotype is an observable trait in an individual. Examples include eye colour, hair colour, or a trait of a disease. There is strong interest in discovering the relationships between genotype and phenotype, as this can enable the attempted prediction of the risk of a disease occurring in an individual, based on their genetic makeup. SNPs are often tested for associations with phenotypes. The response variables we use are (case-control) phenotype data.</p> <p>The genetics data described above is called genotype data, which (as mentioned) is often used in modelling to uncover associations (and indeed could be used in our proposed pipeline). However, the specific disease problem we are tackling uses gene expression data. Gene expression is the process by which information from a gene is used to synthesise a functional gene product, such as proteins or non-coding RNA, which in turn influences a phenotype. Put more simply, gene expression is the basic process through which a genotype results in a phenotype or observable trait. By measursing the strength of the gene expressions in an individual (which is what our data matrix  will contain) we can then discover associations between the genes and a disease outcome (our response ).</p> <p>A final topic of interest is the grouping structure of genes. Genes are often grouped into pathways, which are sets of genes that work together to perform a specific function. This grouping information can be useful in modelling, as it can help to identify which pathways are associated with a disease and can allow a model to utilise grouping information about the genes. Some of the models we apply later on will use this grouping information.</p>"},{"location":"1-chapter-1/#downloading-genetics-data","title":"Downloading genetics data","text":"<p>First, we load a few useful packages: <pre><code>library(GEOquery)\nlibrary(GSA)\nlibrary(Biobase)\n</code></pre> To download the datasets, we can use the helpful <code>getGEO</code> function from the <code>GEOquery</code> package loaded above. This function is able to download a GEO object (which will contain the data needed for fitting models) from the National Center for Biotechnology Information (NCBI) database [L1].  The NCBI database contains many different biomedical and genomic datasets. We are interested in the Gene Expression Omnibus (GEO) datasets, which can be found through the search function, although websites exist which have indexed the datasets [L2].  In our example, we want to investigate whether we can predict cases of inflammatory bowel diseases (Ulcerative colitis and Crohn's). Searching the NCBI website for 'Ulcerative colitis and Crohn's disease comparison' yields the dataset [L3]. Using the dataset ID, we can download the dataset using <pre><code>raw_data &lt;- getGEO('GDS1615')\n</code></pre> This is an object of class <code>GDS</code>, so it does not behave as a normal R object would. To make it easier to manipulate the object, we convert it to a <code>eSet</code> class (ExpressionSet, which is a popular Biobase object) <pre><code>eset_data &lt;- GDS2eSet(raw_data, do.log2 = TRUE)\n</code></pre> We also used this opportunity to log transform the data. A Log2 transformation is commonly applied to gene expression data to stabilize the variance across the range of expression values and to make the data more normally distributed, which is a useful property in statistical modelling.</p> <p>Now we can see that this object behaves much more like a traditional R object, with three elements: <code>sample</code>, <code>disease.state</code>, <code>description</code>. Let's see what these are: <pre><code>head(eset_data$sample)\n</code></pre> This contains the sample ID for the patients, which we will not be using. <pre><code>table(eset_data$disease.state)\n</code></pre> As the name suggests, this contains the disease state of the patients. You can see that there are three levels, with 42 control patients and 85 patients suffering from an inflammatory bowel disease. This is our response variable, , which we will need to encode later to 0 and 1. <pre><code>head(eset_data$description)\n</code></pre> This contains the description of the samples, which we will also not be using.</p> <p>Not every genetics dataset downloaded from NCBI will have the same objects.</p> <p>Q1: download the genetics dataset with the ID <code>GDS807</code>. Do you get the same objects? </p>"},{"location":"1-chapter-1/#processing-pipeline","title":"Processing pipeline","text":"<p>Next, we need to process the data to prepare it for model fitting. As mentioned in the introduction to this section, we want to extract three objects:</p> <ol> <li>The input data matrix, .</li> <li>The response vector, .</li> <li>The grouping structure indexes.</li> </ol>"},{"location":"1-chapter-1/#data-matrix","title":"Data matrix","text":"<p>The data matrix is extracted using the <code>exprs</code> function from the <code>Biobase</code> package, which extracts the gene expression data. Notice that this was not an object we saw from the <code>eset_data</code> object. </p> <p>This demonstrates how working with genetics data in R is not always straightforward. The data is often stored in complex objects, and it is necessary to understand the structure of the object to extract the data needed for analysis. <pre><code>X &lt;- t(exprs(eset_data))\n</code></pre> We also transpose the matrix to get it into the traditional regression format (rows x columns), given by  <pre><code>dim(X)\n</code></pre> where we see that we have 60 samples (rows) and 22575 genes (columns). We can see a snippet of the data <pre><code>X[1:5,1:5]\n</code></pre> which shows the first 5 samples and the first 5 genes. We need to check for missingness, as penalised regression models do not work with missing data (which we would need to remove or impute). There are many ways to check for missingness - one great way is to use the <code>aggr</code> function from the <code>VIM</code> package (there are no missing values in this case, but see Q2 for an example use). </p> <p>In our case, we can simply check the total amount of missing values <pre><code>sum(is.na(X))\n</code></pre> We are fortunate that the data has no missingness and is already well-processed, but sometimes this is not the case.</p> <p>Our data matrix is now ready.</p> <p>Q2: create the data matrix for the dataset <code>GDS807</code>.</p>"},{"location":"1-chapter-1/#disease-state-vector","title":"Disease state vector","text":"<p>We already extracted the response above, but we now assign it to a new variable name and encode it to TRUE/FALSE. We can check the different levels of the response <pre><code>levels(pData(eset_data)$disease.state)\nunique(as.numeric(pData(eset_data)$disease.state))\n</code></pre> We can see that 2 identifies control patients. Therefore, we can assign 2 as FALSE <pre><code>y &lt;- (as.numeric(pData(eset_data)$disease.state) != 2)\n</code></pre> The encoding of TRUE/FALSE is equivalent to a 0,1 encoding. Our response is now ready.</p>"},{"location":"1-chapter-1/#grouping-structure-index","title":"Grouping structure index","text":"<p>This is probably the most challenging part of the processing pipeline, as we need to group the genes into their pathways. </p> <p>To group genes into their pathways, we use the major collections of gene sets of the Human Molecular Signatures Database (MSigDB) [L4].  There are nine such collections, C1-C8 and H, each containing pathways that perform different tasks. For this dataset, we will use the C3 pathway, which are the regulatory target gene sets (gene sets representing potential targets of regulation by transcription factors or microRNAs).</p> <p>We can download the gene set (the .gmt file) from the MSigDB website [L4]. The gene sets are updated regularly and an account is needed to download them.  For those reasons, the C3 .gmt file has been provided in the GitHub repository (<code>data/gene-sets</code>). </p> <p>We load the file using <pre><code>geneset_data &lt;- GSA.read.gmt(\"data/gene-sets/c3.all.v2023.2.Hs.symbols.gmt\")\n</code></pre> This contains three elements:</p> <ol> <li><code>genesets</code>: this is a list, where each list entry is the the names of the genes for a particular gene set.</li> <li><code>geneset.names</code>: the names of the gene sets.</li> <li><code>geneset.description</code>: links which give descriptions of each gene set.</li> </ol> <p>We need to match the genes in  to the genes in the gene sets in the <code>geneset_data</code> object.  We now match the genes to the gene sets. To do this, we use the <code>match</code> R function.  <pre><code>gene_identifiers &lt;- Table(raw_data)[,2] # these are the gene names in X\nnum_geneset &lt;- length(geneset_data$genesets) # the number of gene sets\n\nindex &lt;- rep(0,length(gene_identifiers)) # these will be the group indexes, which will be found by matching the genes to the gene sets\n\nfor(i in 1:num_geneset){ # loop match over each gene set\n  matched_index &lt;- match(geneset_data$genesets[[i]],gene_identifiers)\n  index[matched_index] &lt;- i\n}\n</code></pre> We check which genes have been matched to a gene set (note that not every gene will have a match, in which case we drop those genes) <pre><code>ind_include &lt;- which(index != 0)\nlength(ind_include)\n</code></pre> We can see that about 12000 genes have been matched, just over half of the genes present in . We now reconstruct  with only the matched genes</p> <p><pre><code>matched_gene_names &lt;- gene_identifiers[ind_include]\nX &lt;- X[,ind_include]\ndim(X)\n</code></pre> We now need to reperform the matching with the remaining genes <pre><code>group_index &lt;- rep(0,ncol(X))\nfor(i in 1:num_geneset){ # iterate over each gene set\n  for(j in 1:length(geneset_data$genesets[[i]])){ # iterate over each gene in the gene set\n    change.ind &lt;- match(geneset_data$genesets[[i]][j],matched_gene_names) # matching gene to gene set\n    if(!is.na(change.ind)){ # if the gene is in the gene set\n      if(group_index[change.ind] == 0){ # if the gene has not been assigned a group index\n        group_index[change.ind] &lt;- i\n      }\n    }\n  }\n}\n\nhead(group_index)\n</code></pre> This double loop is not computationally the most ideal solution and might be slow for large examples. To speed this up, parallelisation could be used.</p> <p>We are almost done. The final step is to reset the group indexing so that it runs from 1,2,... Many of the models we will use do not work unless this is done. We reset the indexing by looping over each gene set and assigning the group index to the genes in the gene set. <pre><code>grp_ids = data.frame(col_id = 1:length(unique(group_index)),grps=unique(group_index))\nordered_group_indx = rep(0,length(group_index))\nfor (i in 1:length(group_index)){\n  ordered_group_indx[i] = which(grp_ids$grps == group_index[i])\n}\n</code></pre></p>"},{"location":"1-chapter-1/#save-dataset","title":"Save dataset","text":"<p>We can now save the cleaned dataset (we will save it as an RDS file) <pre><code>setwd(\"data\")\ndata = list()\ndata$X = X\ndata$y = y\ndata$groups = ordered_group_indx\nsaveRDS(data,\"colitis-data-c3.RDS\")\n</code></pre></p> <p>Q3: run the full pipeline with the dataset <code>GDS807</code>.</p>"},{"location":"1-chapter-1/#links","title":"Links","text":"<ul> <li>[L1] https://www.ncbi.nlm.nih.gov/gds</li> <li>[L2] https://cola-gds.github.io/</li> <li>[L3] https://www.ncbi.nlm.nih.gov/sites/GDSbrowser?acc=GDS1615</li> <li>[L4] https://www.gsea-msigdb.org/gsea/msigdb/human/collections.jsp</li> </ul>"},{"location":"1-chapter-1/#references","title":"References","text":"<ul> <li>[R1] Sano Genetics. SNP of the week, 2019. URL https://medium.com/@sanogenetics/ snp-of-the-week-58e23927c188.</li> </ul>"},{"location":"2-chapter-1-solutions/","title":"Chapter 1 solutions","text":""},{"location":"2-chapter-1-solutions/#q1-download-the-genetics-dataset-with-the-id-gds807-do-you-get-the-same-objects","title":"Q1: download the genetics dataset with the ID <code>GDS807</code>. Do you get the same objects?","text":"<p>This dataset contains genetic information from patients suffering from breast cancer. We extract the data as before <pre><code>raw_data &lt;- getGEO('GDS807')\neset_data &lt;- GDS2eSet(raw_data, do.log2 = TRUE)\n</code></pre> We notice that an error regarding NaNs is printed. This is due to the log transformation working on negative data, but is not a concern as we will filter this out anyway. </p> <p>We can check the objects in the <code>eset_data</code> object <pre><code>head(eset_data$sample)\ntable(eset_data$disease.state)\nhead(eset_data$description)\n</code></pre> We have the same objects for the colitis data, but this time we have 32 controls and 28 patients suffering from the disease. </p>"},{"location":"2-chapter-1-solutions/#q2-create-the-data-matrix-for-the-dataset-gds807","title":"Q2: create the data matrix for the dataset <code>GDS807</code>.","text":"<p>As before, we run <pre><code>X &lt;- t(exprs(eset_data))\n</code></pre> We do an initial check for missingness <pre><code>sum(is.na(X))\n</code></pre> and observe that there is a high amount of missingness. A nice way to profile missingness in a data matrix is to use the <code>aggr</code> function from the <code>VIM</code> package. Running the function on the full data matrix will be too computationally intensive, so we run it on a small subset to illustrate the usefulness of the function. <pre><code>library(VIM)\naggr(X[,1000:1020])\n</code></pre></p> <p></p> <p>We observe on the left plot that there is a fair amount of variance in the amount of missing values between the genes. Some genes have no missing values, while others have only missing values. On the right plot, we can see which combinations of missingness occur most often. The most frequently occuring combination is of ten genes. This plot is very useful for finding patterns of missingness.</p> <p>We will remove the missing values at the end of this section.</p>"},{"location":"2-chapter-1-solutions/#q3-run-the-full-pipeline-with-the-dataset-gds807","title":"Q3: run the full pipeline with the dataset <code>GDS807</code>.","text":"<p>We now complete the data pipeline with this dataset.</p>"},{"location":"2-chapter-1-solutions/#response","title":"Response","text":"<p>Checking the numerical assignment: <pre><code>levels(pData(eset_data)$disease.state)\nunique(as.numeric(pData(eset_data)$disease.state))\n</code></pre> Unlike in the colitis case, here a value of 2 indicates a patient with the disease. So we assign the response as <pre><code>y &lt;- (as.numeric(pData(eset_data)$disease.state) != 2)\n</code></pre> The encoding of TRUE/FALSE is equivalent to a 0,1 encoding. We could just as easily have assigned it the other way round - it would not change the modelling process. Our response is now ready. </p>"},{"location":"2-chapter-1-solutions/#grouping-structure-index","title":"Grouping structure index","text":"<p>As before, we are using the gene set collections here. This time, we use the C8 collection (although you can also use the C3, or any other). The C8 collection contains cell type signature gene sets. The file is provided in the GitHub repository (<code>data/gene-sets</code>). <pre><code>geneset_data &lt;- GSA.read.gmt(\"data/gene-sets/c8.all.v2023.2.Hs.symbols.gmt\")\n</code></pre></p> <p>We perform the matching as before.  <pre><code>gene_identifiers &lt;- Table(raw_data)[,2] # these are the gene names in X\nnum_geneset &lt;- length(geneset_data$genesets) # the number of gene sets\n\nindex &lt;- rep(0,length(gene_identifiers)) # these will be the group indexes, which will be found by matching the genes to the gene sets\n\nfor(i in 1:num_geneset){ # loop match over each gene set\n  matched_index &lt;- match(geneset_data$genesets[[i]],gene_identifiers)\n  index[matched_index] &lt;- i\n}\n</code></pre> We check which genes have been matched to a gene set (note that not every gene will have a match, in which case we drop those genes) <pre><code>ind_include &lt;- which(index != 0)\nmatched_gene_names &lt;- gene_identifiers[ind_include]\nX &lt;- X[,ind_include]\ndim(X)\n</code></pre></p> <p>We now need to reperform the matching with the remaining genes <pre><code>group_index &lt;- rep(0,ncol(X))\nfor(i in 1:num_geneset){ # iterate over each gene set\n  for(j in 1:length(geneset_data$genesets[[i]])){ # iterate over each gene in the gene set\n    change.ind &lt;- match(geneset_data$genesets[[i]][j],matched_gene_names) # matching gene to gene set\n    if(!is.na(change.ind)){ # if the gene is in the gene set\n      if(group_index[change.ind] == 0){ # if the gene has not been assigned a group index\n        group_index[change.ind] &lt;- i\n      }\n    }\n  }\n}\n\nhead(group_index)\n</code></pre></p>"},{"location":"2-chapter-1-solutions/#removing-missingness","title":"Removing missingness","text":"<p>As a final step, we need to remove the missing values. To do this, we need two functions. The first function calculates the proportion of missingness for each gene (column).  The second function imputes the missing values with the mean of the non-missing values for each gene. <pre><code>prop.missing &lt;- function(X){ \n  apply(X,2,function(x){mean(is.na(x))})\n}\n\nmean.impute &lt;- function(X){\n  means &lt;- apply(X,2,function(x){mean(x[which(!is.na(x))])})\n  for(i in 1:ncol(X)){\n    ind &lt;- which(is.na(X[,i]))\n    X[ind,i] &lt;- means[i]\n  }\n  return(X)\n}\n</code></pre></p> <p>We now apply these two functions to the data matrix. We will remove all genes with more than 50% missingness.  For the remaining genes, we will mean impute any missing values.  <pre><code>prop.m &lt;- prop.missing(X)\nremove.ind &lt;- which(prop.m &gt; 0.5)\nimp.X &lt;- mean.impute(X[,-remove.ind])\nX &lt;- imp.X\ndim(X)\n</code></pre></p> <p>As we have removed variables (and this should be checked regardless), we need to reset the group indexing so that it runs from 1,2,... Many of the models we will use do not work unless this is done. We reset the indexing by looping over each gene set and assigning the group index to the genes in the gene set. <pre><code>group_index = group_index[-remove.ind]\ngrp_ids = data.frame(col_id = 1:length(unique(group_index)),grps=unique(group_index))\nordered_group_indx = rep(0,length(group_index))\nfor (i in 1:length(group_index)){\n  ordered_group_indx[i] = which(grp_ids$grps == group_index[i])\n}\n</code></pre></p>"},{"location":"2-chapter-1-solutions/#save-dataset","title":"Save dataset","text":"<p>We can now save the cleaned dataset (we will save it as an RDS file) <pre><code>setwd(\"data\")\ndata = list()\ndata$X = X\ndata$y = y\ndata$groups = ordered_group_indx\nsaveRDS(data,\"cancer-data-c8.RDS\")\n</code></pre></p>"},{"location":"3-chapter-2/","title":"Chapter 2: Motivation, penalised regression, and the lasso","text":"<p>This chapter will provide the (genetics) motivation for the problem we are trying to solve. It will then provide mathematical descriptions of the penalised regression methods that we will use. R code is provided for implementing the methods to simple examples, which will be expanded to the genetics data in Chapter 3. During this chapter, think about how these methods can be adapted to working with genetics data, which we will then cover in Chapter 3.</p>"},{"location":"3-chapter-2/#motivation-optional","title":"Motivation (optional)","text":"<p>The genetics background for understanding the motivation was described in Chapter 1.  Here, we give a detailed description of the problem we are trying to solve, from a genetics point of view. The mathematical formulation of the problem is described in the next section.</p> <p>A key problem in genetics is to identify which genotypes (genetic markers) are associated with phenotypes (trait). Early approaches of identifying these genotypes used univariate analysis, where each gene was independently tested for association with the outcome. In these analyses, simple statistical tests, such as the t-test, are used to calculate p-values on the significance of single gene expressions on a phenotype [R1].  However, genes sit within groups (called pathways), and so these approaches do not fully utilise the biological information available.  We might expects genes in the same pathways to behave in similar ways and interact with each other.</p> <p>Pathway analysis, also known as gene set analysis, attempts to make use of the information about the biological set-up of the genes being investigated, with the hope of this leading to more accurate models.  As such, the aim of pathway analysis is to identify pathways, which are groups of genes, that have an association with a particular phenotype.  Popular examples of pathway analysis methods include over-representation analysis (ORA), functional class scoring (FCS) methods, and pathway topology analysis (PTA) [R1].</p> <p>The problem of selecting relevant pathways can be seen as both an individual variable and group selection problem - we want to find both the relevant pathways and the genes within those pathways. Methods for variable selection are countless, ranging from simple linear regression to more sophisticated approaches, such as neural networks [R2, R3]. </p> <p>In pathway analysis, we use datasets with more genes () than individuals () to which they belong. As such, pathway analysis falls under the  umbrella of problems (this is the field of high-dimensional statistics), commonly known as short, fat data problems.  Additionally, as only a small fraction of genes and pathways tend to be disease-associated, we seek methods which encourage sparse solutions [R4, R5].</p> <p>A particular family of methods which are often used to solve  problems and generate sparse solutions are penalised regression methods, which we discuss next.</p>"},{"location":"3-chapter-2/#penalised-regression","title":"Penalised regression","text":"<p>Note on notation:  will refer to the coefficient for a variable , whilst  refers to the vector of coefficients for the variables in group  (of which there are ).</p> <p>In the traditional linear regression setting, we have response data , of dimension , and input data , of dimension , and a corresponding model given by , where .  With the rise of big data, we are increasingly tackling datasets where  (which falls under the umbrella of high-dimensional statistics), including in cyber security, biomedical imaging, and as mentioned above, pathway analysis.  In such cases, there are insufficient degrees of freedom to estimate the full model. Indeed, the solution to linear regression (ordinary least squares) can not be computed for high-dimensional datasets, as the inverse of  does not exist (which is needed for the solution).</p>"},{"location":"3-chapter-2/#lasso","title":"Lasso","text":"<p>As a solution for dealing with such a situation, [R6] introduced the least absolute shrinkage and selection operator (lasso). The lasso performs variable selection by regularisation; that is, it minimises a loss function subject to some constraints.  Formally, the lasso finds  estimates by solving  where   is the  norm and  is the  norm. The parameter  defines the amount of sparsity in the fitted model. </p> <p>If  is large, very few coefficients will be non-zero and the model will be very sparse. On the other hand, small values of  will lead to a model with many non-zero coefficients, eventually leading to , where we recover the ordinary least squares solution. We can pick  subjectively ourselves, but the most common approach is to fit models for different values of  and select the best one using cross-validation (the one with the lowest error).</p> <p>The approach generates sparse solutions by shrinking some coefficients and setting others to 0 exactly, retaining the desirable properties of subset selection and ridge regression [R6] - this is shown in the figure below. The figure shows the solutions of the lasso (left) and ridge regression (right) for . The blue regions are the constraint regions and the red eclipses are the contour lines of the least squared errors function. The solutions are given by where the contours hit the constraint region.</p> <p></p> <p>As a consequence of the diamond shape of the lasso constraint region, if the solution occurs at the corner, the corresponding parameter  is set exactly to 0, which is not possible with the ridge constraint region [R7]. For , the diamond is a rhomboid, which has many edges, and we retain this desirable property of the lasso.</p> <p>We can implement the lasso in R using the <code>glmnet</code> package, which is one of the most widely used packages. To run the lasso, we create some synthetic Gaussian data <pre><code>set.seed(2)\nX &lt;- matrix(rnorm(100 * 20), 100, 20)\nbeta &lt;- c(rep(5,5),rep(0,15))\ny &lt;- X%*%beta + rnorm(100)\n</code></pre></p> <p>We can run the lasso using the default set-up, specifying only that we want to run the model for 20 values of . <pre><code>library(glmnet)\nfit &lt;- glmnet(x = X, y = y, family = \"gaussian\", nlambda = 20, lambda.min.ratio = 0.1)\n</code></pre></p> <p>Q1: run the same model again but without standardising. What do you observe? </p> <p>Q2: what happens if we do not specify how many 's we want?</p> <p>Q3 (optional): the glmnet documentation states that there is a parameter  that we can change. What does this do?</p> <p>Q4 (optional): look at the glmnet documentation - which parameters might be interesting to vary?</p> <p>The model will be run from the value of  that generates a null model (no active variables) to the final value of  that is a specified proportion of the first value (the value of <code>lambda.min.ratio</code>). We can visualise the path of  values, which take a log-linear shape (by design) <pre><code>plot(fit$lambda,type=\"b\", ylab=\"Value\")\n</code></pre></p> <p></p> <p>We can also visualise some of the active coefficients using <pre><code>plot(fit)\n</code></pre></p> <p></p> <p>and we can observe that the coefficients become larger in absolute value as the value of  become smaller. So we now have 20 lasso models and we want to pick a single one to use for prediction. The most popular option is to use cross-validation: <pre><code>fit.cv &lt;- cv.glmnet(x = X, y = y, family = \"gaussian\", nlambda = 20, lambda.min.ratio = 0.1)\nprint(fit.cv)\n</code></pre> In the print-out, there are actually two different \"minimums\" defined. The first is <code>min</code>, which is the absolute minimum error obtained by any of the models. The other is <code>1se</code>, which is the simplest model that is within one-standard error of the minimum. This is often used as a more conservative estimate of the best model, as it is less likely to be overfitting the data. Using the <code>1se</code> model, we can compare the obtained coefficients to the true beta values <pre><code>cbind(beta,fit.cv$glmnet.fit$beta[,19])\n</code></pre> and we find the lasso does very well. It correctly picks out the first five variables as those with a signal and correctly sets the rest to zero. In Chapter 3, where we apply these methods to real data, we will instead pick  to be the value that minimises the error on a test data set.</p> <p>We can now use these fitted coefficients to predict . To fairly test the accuracy of the model, we first need to generate some new data (that uses the same  signal) <pre><code>set.seed(3)\nX_new &lt;- matrix(rnorm(10 * 20), 10, 20)\ny_new &lt;- X_new%*%beta + rnorm(10)\n</code></pre> and we can predict the new s values using the lasso model <pre><code>cbind(y_new,predict(object = fit.cv, newx = X_new, s = \"lambda.1se\"))\n</code></pre> and we find our predictions are not too bad. We can get a prediction error score (using the mean squared error) <pre><code>mean((y_new-predict(object = fit.cv, newx = X_new, s = \"lambda.1se\"))^2)\n</code></pre></p> <p>Q5: instead of using the predict function, manually code a prediction.</p> <p>Q6: compare the predictions of the 1se model against the min model.</p>"},{"location":"3-chapter-2/#group-lasso-glasso","title":"Group lasso (gLasso)","text":"<p>We mentioned in the motivation that genes come in groups and that we would like to utilize this grouping information.  To that end, [R8] adapted the lasso approach to the problem of selecting grouped variables by introducing the group lasso (gLasso).  Let  be non-overlapping groups of variables (all groups are assumed to be non-overlapping), then the solution is given by  where  is the number of variables in group . If the size of each group is 1, then we recover the lasso solution.  The group lasso creates sparsity at a group level by shrinking whole groups exactly to 0, so that each variable within a group is shrunk to 0 exactly.  The group lasso has found widespread use in finding significant genetic variants [R9, R10].</p> <p>There are two main R packages for fitting the group lasso: <code>grplasso</code> and <code>gglasso</code>.  We will use the <code>grplasso</code> package, as it contains additional features. To use the group lasso, we need grouping indexes for the variables.  In genetics, natural groupings are found through pathways (which we saw in Chapter 1). If a natural grouping does not exist, algorithms that perform some form of grouping can be used (such as k-means clustering).  For simplicity, in this case we will manually assign the variables into groups of size 4. <pre><code>groups &lt;- rep(1:5,4)\n</code></pre> The <code>grplasso</code> package does not automatically calculate a path of  values, so we compute those first, starting with the first  value using the <code>lambdamax</code> function from the package. Additionally, the package does not implement an intercept and standardisation in the same way as <code>glmnet</code>, so we do this manually. In general, when working with penalised regression models, it is a good idea to fit an intercept and apply standardisation. In general, when in doubt about whether a package performs standardisation properly, a useful check is to compare it to <code>glmnet</code>, as this is the gold standard of R packages (most penalised regression models reduce to the lasso under certain hyperparameter choices, so a direct comparison is possible most of the time).</p> <p>Q7: compare the <code>grplasso</code> package to <code>glmnet</code> to see if standardisation works properly?</p> <p><pre><code>library(grplasso)\n# standardise data\nX_gl &lt;- t(t(X) - apply(X, 2, mean)) # center data\nX_gl &lt;- t(t(X_gl) / apply(X_gl, 2, sd)) # scale data\nX_gl &lt;- cbind(1, X_gl) # add intercept\ngroups_gl &lt;- c(NA, groups) # we add NA as the intercept does not belong to a group\n\n# generate lambda sequence\nlambda_max_group &lt;- lambdamax(X_gl, y, groups_gl, standardize = FALSE, center = FALSE) # finding lambda max, we can ignore the warning\nlambdas_gl &lt;- exp(seq( # calculating the full lambda sequence\n  from = log(lambda_max_group),\n  to = log(lambda_max_group * 0.1), # the equivalent of lambda.min.ratio in glmnet\n  length.out = 20 # how many lambdas we want\n))\n\n# fit model\nglasso_model &lt;- grplasso(x = X_gl, y = y, index = groups_gl, lambda = lambdas_gl, standardize = FALSE, center = FALSE, model = LinReg())\n</code></pre> Comparing the model coefficients to the true ones (for the most saturated model, removing the intercept value): <pre><code>cbind(beta, glasso_model$coefficients[-1,20])\n</code></pre> and using it to predict the output (you will notice we have to do a few things manually, such as adding the intercept to <code>X_new</code>, because, as mentioned, many R packages do not have as complete features as <code>glmnet</code>) <pre><code>mean((y_new-predict(object = glasso_model, newdata = cbind(1,X_new))[,20])^2)\n</code></pre> This case demonstrates a real limitation of the group lasso. In our manual grouping, we placed a signal variable (one that is not zero) in each group, forcing the group lasso to pick each of those groups as non-zero. The group lasso picks whole groups, so that it is in turn forced to make every variable in those groups non-zero. In the next section, we explore a solution to this issue, which is especially limiting in genetics.</p> <p>Q8: set the group indexing so that the signal variables are all in the same group. What do you observe?</p>"},{"location":"3-chapter-2/#sparse-group-lasso-sgl","title":"Sparse-group lasso (SGL)","text":"<p>Using the group lasso for pathway analysis would require the assumption that all genes in a significant pathway are also significant, not allowing for additional sparsity within a group [R11]. So, one may wish to have sparsity at both the group (pathway) and variable (gene) level. </p> <p>To fulfil this wish, [R9] introduced the sparse-group lasso (SGL), which combines traditional lasso with the group lasso to create models with bi-level sparsity.  The solution to SGL is given by  where  controls the level of sparsity between group and variable sparsity. If , we recover the lasso, and  recovers the group lasso. The figure below shows how SGL is a convex combination of these two approaches: contour lines are shown for the group lasso (dotted), lasso (dashed), and sparse-group lasso (solid) for  .</p> <p></p> <p>We can implement SGL using the <code>SGL</code> R package. We now need to specify the additional  parameter. This is normally set subjectively, because if we were to include this in a cross-validation tuning regime, we would need to do a grid search with , which is expensive. [R11] suggest the value of , giving mostly the lasso penalty with only a bit of the group lasso. This is a sensible suggestion, as it allows the model to use the strengths of the lasso whilst avoiding the limitation of the group lasso (having to select all variables as significant). <pre><code>library(SGL)\nsgl_model &lt;- SGL(list(x=X,y=y), groups, type = \"linear\", nlam = 20, min.frac = 0.1, alpha = 0.95)\n</code></pre> As before, we can look at the coefficients <pre><code>cbind(beta, sgl_model$beta[,20])\n</code></pre> We observe inflated coefficients in comparison to the true values. </p> <p>Q9 (optional): can you figure out why we get inflated values?</p> <p>Now prediction: <pre><code>mean((y_new-predictSGL(x = sgl_model, newX = X_new, lam = 20))^2)\n</code></pre> We can observe how the prediction changes as a function of : <pre><code>preds = rep(0,20)\nfor (i in 1:20){\n    preds[i] = mean((y_new-predictSGL(x = sgl_model, newX = X_new, lam = i))^2)\n}\nplot(preds,type=\"b\", ylab=\"Error\")\n</code></pre></p> <p></p> <p>and we see that the prediction improves as  decrease (that is, as the coefficients become larger and more variables enter the model).</p> <p>Q10: what happens to the predictive score if we allow  to decrease even further?</p> <p>Q11: vary  in the region . What do you observe?</p> <p>Q12: can you use the <code>SGL</code> R package to fit the lasso and group lasso?</p> <p>In pathway analysis, the proportion of relevant pathways, amongst all pathways, is often very low. Additionally, the proportion of relevant genes within a particular pathway is also low. As such, the SGL model can provide the required level of sparsity at both the pathway and the individual gene level. Indeed, SGL has already been applied to detecting significant genetic variants in [R11]. </p>"},{"location":"3-chapter-2/#slope-optional","title":"SLOPE (optional)","text":"<p>In genetic data, genes are usually highly correlated with one another. We can visualise this by looking at a correlation plot for the colitis dataset that we prepared in Chapter 1. <pre><code>library(reshape2)\nlibrary(ggplot2)\ngene_data &lt;- readRDS(\"colitis-data-c3.RDS\")\n\ncorr_mat = cor(gene_data[[1]][,sample(1:ncol(gene_data[[1]]),size = 250)]) # create correlation matrix of subset\n\nmelted_corr_mat &lt;- melt(corr_mat) # get into correct format for ggplot\n\nggplot(data = melted_corr_mat, aes(x = Var1, y = Var2, fill = value)) +\n  geom_tile() +\n  scale_fill_gradient2(low = \"blue\", high = \"red\", mid = \"white\", \n                       midpoint = 0, limit = c(-1, 1), space = \"Lab\", \n                       name=\"Correlation\") +\n  theme_minimal() + \n  theme(axis.text = element_blank(),  # Remove axis text\n        axis.title = element_blank(), # Remove axis titles\n        panel.grid.major = element_blank(),  # Remove grid lines\n        panel.border = element_blank(),  # Remove border\n        panel.background = element_blank(),  # Remove background\n        axis.ticks = element_blank()) +  # Remove axis ticks\n  coord_fixed()\n</code></pre></p> <p></p> <p>We observe many regions of very high corraltion. </p> <p>The lasso is able to recover the support of a model if the covariates are not too correlated, but struggles for highly correlated data [R14], making it suboptimal for use with genetics data. Additionally, when discovering associated genes, we want to reduce the number of false discoveries; that is, the false discovery rate (FDR). These two aspects motivated the proposal of the SLOPE method.</p> <p>The sorted l-one penalised estimation (SLOPE) method is an extension of the lasso and was proposed by [R15]. It is considered to be bridge between the lasso and FDR-control in multiple testing. It can also been seen as an adaptive version of the lasso, in which each coefficient is given its own penalty. </p> <p>The solution is given by  where  and . Here,  are the adaptive weights and  acts as  did in the lasso. The notation can be somewhat confusing between these methods, especially as different authors use different notations (I have tried to clarify this in the notation section below). In SLOPE, the highest variables (in absolute value) are matched to the largest penalty. The idea is that it makes it harder for variables to be non-zero, so that any that do, we can be fairly certain that they actually are signals (which reduces the FDR). When all of the penalty terms are identical, that is , then SLOPE reduces to the lasso. </p> <p>The link to FDR control comes via the Benjamini-Hochberg (BH) procedure (which is a popular FDR controlling procedure), through the choice of the adaptive penalty weights, . The BH critical values are used as the choice of penalisation parameters, so that , where  is the FDR level and  is the quantile function of a standard normal distribution.</p> <p>We can implement SLOPE in R using the <code>SLOPE</code> package</p> <p><pre><code>library(SLOPE)\nslope_model = SLOPE(x = X, y = y, family = \"gaussian\", alpha_min_ratio = 0.1, path_length = 20)\n</code></pre> Here, <code>alpha_min_ratio</code> acts as <code>lambda.min.ratio</code> in the <code>glmnet</code> function. We can first look at the penalty weights <pre><code>plot(slope_model$lambda, type = \"b\", ylab=\"Value\")\n</code></pre></p> <p></p> <p>As before, we can look at the coefficients</p> <p><pre><code>cbind(beta, slope_model$coefficients[-1,,20])\n</code></pre> and use the package for prediction <pre><code>mean((y_new - predict(object = slope_model, x = X_new)[,20])^2)\n</code></pre></p>"},{"location":"3-chapter-2/#group-slope-gslope","title":"Group SLOPE (gSLOPE)","text":"<p>The SLOPE method was also extended to the group setting by [R15]. Group SLOPE (gSLOPE) is defined by  where  acts as  in SLOPE, and where  and . The approach applies larger penalties to groups with greater effects. As is the case for SLOPE, if , then group SLOPE reduces to the group lasso.</p> <p>To fit gSLOPE, we use the <code>grpSLOPE</code> R package (notice that it does not have as many features as the other packages used - for instance, we can only specify one value of , rather than a path as before):</p> <p><pre><code>library(grpSLOPE)\ngslope_model = grpSLOPE(X = X, y = y, group = groups, sigma = 0.01, fdr = 0.1)\n</code></pre> We can look at the penalty weights <pre><code>plot(gslope_model$lambda, type = \"b\", ylab = \"Value\")\n</code></pre></p> <p></p> <p>and the coefficients (where the group penalty limitation is seen again) <pre><code>cbind(beta, gslope_model$beta)\n</code></pre> and use it for prediction <pre><code>mean((y_new - predict(object = gslope_model, newdata = X_new))^2)\n</code></pre></p>"},{"location":"3-chapter-2/#sparse-group-slope-sgs","title":"Sparse-group SLOPE (SGS)","text":"<p>It has been further extended to the sparse-group setting by [R16] to form the sparse-group SLOPE (SGS) model. It is defined by  where</p> <ul> <li>  acts as the traditional lasso penalty term. By varying , we are able to create a pathwise solution, as is done in the lasso approach. </li> <li>  is a convex combination of SLOPE and group SLOPE, as in SGL.</li> <li>  are the adaptive penalty weights applied to group . They are equivalent to the  penalties used in group SLOPE. As is done for group SLOPE, we have  and .</li> <li>  are the adaptive penalty weights applied to the individual variables. They are equivalent to the  penalties used in SLOPE. We have that  and .</li> </ul> <p>If  and , then SGS reduces to SGL. SGS is suitable for pathway analysis, as not only does it allow for both genes and pathways to be selected, but it will also control the FDR of the solution at a bi-level.</p> <p>We can visualise SGS as being a combination of SLOPE and gSLOPE (in the figure below).</p> <p></p> <p>SGS can be implemented in R using the <code>sgs</code> R package <pre><code>library(sgs)\nsgs_model = fit_sgs(X = X, y = y, type = \"linear\", groups = groups, lambda = 0.15, alpha = 0.95)\n</code></pre> and the coefficients (where the group penalty limitation is no longer present) <pre><code>cbind(beta, sgs_model$beta[-1])\n</code></pre> and use it for prediction <pre><code>mean((y_new - predict(object = sgs_model, x = X_new))^2)\n</code></pre></p>"},{"location":"3-chapter-2/#notation-optional","title":"Notation (optional)","text":"<p>Generally, we have three components to a penalised regression model:</p> <ol> <li>The penalisation parameter. This is  in the lasso, gLasso, SGL, and SGS,  in SLOPE, and  in gSLOPE. This determines the degree of sparsity in a model. If the value is high, then few variables will be non-zero. This is only ever a single value defined once for the whole model, although often we fit several models for different values of .</li> <li>The penalty weights. The lasso does not have penalty weights explicitly defined, because they are hidden as they are just 1 for each coefficient. As SLOPE and gSLOPE are adaptive, each variable/group has a different penalty weight, defined as . For SGS, as there are additional components needing notation, the penalty weights are set to  for variables and  for groups.</li> <li>The balance parameter (only for models with more than one penalty function). For SGL and SGS, this is  and it defines the 'balance' between the two types of penalties used (the lasso and group lasso for SGL, and SLOPE and gSLOPE for SGS).</li> </ol> <p>The notation used in this chapter has been done to make it consistent with that used in the R functions.</p>"},{"location":"3-chapter-2/#final-prediction-table","title":"Final prediction table","text":"<p>Throughout this chapter, we applied several different methods for predicting a simple dataset. These are collected in the table below for direct comparison (including the optional models). This is only a very limited case, so the prediction scores are not hugely insightful. Additionally, we did not properly tune each model. A proper comparison of the model's predictive performances will be shown in Chapter 3.</p> Model Prediction error Lasso 3.9 gLasso 4.0 SGL 3.6 SLOPE 3.6 gSLOPE 2.2 SGS 2.8"},{"location":"3-chapter-2/#r-package-links","title":"R package links","text":"<ul> <li>glmnet</li> <li>grplasso</li> <li>gglasso</li> <li>SGL</li> <li>SLOPE</li> <li>grpSLOPE</li> <li>sgs</li> </ul>"},{"location":"3-chapter-2/#references","title":"References","text":"<ul> <li>[R1] F. Maleki, K. Ovens, D. J. Hogan, and A. J. Kusalik. Gene Set Analysis: Challenges, Opportunities, and Future Research. Frontiers in Genetics, 11(June):1-16, 2020. ISSN 16648021. doi: 10.3389/ fgene.2020.00654.</li> <li>[R2] G. Heinze, C. Wallisch, and D. Dunkler. Variable selection - A review and recommendations for the practicing statistician. Biometrical Journal, 60(3):431-449, 2018. ISSN 15214036. doi: 10.1002/bimj.201700067.</li> <li>[R3] M. Ye and Y. Sun. Variable selection via penalized neural network: A drop-out-one loss approach. 35th International Conference on Machine Learning, ICML 2018, 13:8922-8931, 2018.</li> <li>[R4] C. Yang, X. Wan, Q. Yang, H. Xue, and W. Yu. Identifying main effects and epistatic interactions from large-scale SNP data via adaptive group Lasso. BMC Bioinformatics, 11(SUPPLL.1):1-11, 2010. ISSN 14712105. doi: 10.1186/1471-2105-11-S1-S18.</li> <li>[R5] Y. Guo, C. Wu, M. Guo, Q. Zou, X. Liu, and A. Keinan. Combining sparse group lasso and linear mixed model improves power to detect genetic variants underlying quantitative traits. Frontiers in Genetics, 10(APR):1-11, 2019. ISSN 16648021. doi: 10.3389/fgene.2019.00271.</li> <li>[R6] R. Tibshirani. Regression Shrinkage and Selection via the Lasso. Journal of the Royal Statistical Society., 58(1):267-288, 1996.</li> <li>[R7] T. Hastie, R. Tibshirani, and J. Friedman. The Elements of Statistical Learning Data Mining, Inference, and Prediction, Second Edition, volume 103. Springer New York, 2nd ed. 20 edition, 2009. ISBN 9780387848587.</li> <li>[R8] M. Yuan and Y. Lin. Model selection and estimation in regression with grouped variables. Journal of the Royal Statistical Society. Series B: Statistical Methodology, 68(1):49-67, 2006. ISSN 13697412. doi: 10.1111/j.1467-9868.2005.00532.x.</li> <li>[R9] J. Li, Z.Wang, R. Li, and R.Wu. Bayesian group lasso for nonparametric varying-coefficient models with application to functional genome-wide association studies. Annals of Applied Statistics, 9 (2):640-664, 2015. ISSN 19417330. doi: 10.1214/15-AOAS808.</li> <li>[R10] M. Lim and T. Hastie. Learning interactions via hierarchical group-lasso regularization. J Comput Graph Stat., 24(3):627-654, 2015. doi: 10.1080/10618600.2014.938812.</li> <li>[R11] Y. Guo, C. Wu, M. Guo, Q. Zou, X. Liu, and A. Keinan. Combining sparse group lasso and linear mixed model improves power to detect genetic variants underlying quantitative traits. Frontiers in Genetics, 10(APR):1-11, 2019. ISSN 16648021. doi: 10.3389/fgene.2019.00271.</li> <li>[R12] N. Simon, J. Friedman, T. Hastie, and R. Tibshirani. A sparse-group lasso. Journal of Compu- tational and Graphical Statistics, 22(2):231-245, 2013. ISSN 10618600. doi: 10.1080/10618600. 2012.681250.</li> <li>[R13] J. Friedman, T. Hastie, and R. Tibshirani. A note on the group lasso and a sparse group lasso. pages 1-8, 2010. URL http://arxiv.org/abs/1001.0736.</li> <li>[R14] L. Jacob, G. Obozinski, and J. P. Vert. Group lasso with overlap and graph lasso. ACM International Conference Proceeding Series, 382, 2009. doi: 10.1145/1553374.1553431.</li> <li>[R15] M. Bogdan, E. van den Berg, C. Sabatti, W. Su, and E. J. Candes. Slope - adaptive variable selection via convex optimization. Annals of Applied Statistics, 9(3):1103-1140, 2015. ISSN 19417330. doi: 10.1214/15-AOAS842.</li> <li>[R16] D. Brzyski, A. Gossmann, W. Su, and M. Bogdan. Group SLOPE - Adaptive Selection of Groups of Predictors. Journal of the American Statistical Association, 114(525):419-433, 2015. ISSN 1537274X. doi: 10.1080/01621459.2017.1411269.</li> <li>[R17] F. Feser and M. Evangelou. Sparse-group SLOPE: adaptive bi-level selection with FDR-control. arXiv preprint arXiv:2305.09467, 2023.</li> </ul>"},{"location":"4-chapter-2-solutions/","title":"Chapter 2 solutions","text":""},{"location":"4-chapter-2-solutions/#q1-run-the-same-model-again-but-without-standardising-what-do-you-observe","title":"Q1: run the same model again but without standardising. What do you observe?","text":"<p>As before, we run <pre><code>set.seed(2)\nX &lt;- matrix(rnorm(100 * 20), 100, 20)\nbeta &lt;- c(rep(5,5),rep(0,15))\ny &lt;- X%*%beta + rnorm(100)\n\nlibrary(glmnet)\nfit &lt;- glmnet(x = X, y = y, family = \"gaussian\", nlambda = 20, lambda.min.ratio = 0.1)\n</code></pre> We also run the model without standardising <pre><code>fit_ns &lt;- glmnet(x = X, y = y, family = \"gaussian\", nlambda = 20, lambda.min.ratio = 0.1, standardize = FALSE)\ncbind(fit$beta[,20],fit_ns$beta[,20])\n</code></pre> The solutions are similar but not quite the same. A lot of things happen in the background when standardisation occurs. Without getting into too much detail, standardisation scales , to ensure we get similar solutions, but this scaling is only approximate, hence the difference. Generally, it is recommended to use standardisation (and when using packages as complete as <code>glmnet</code>, to let the package handle it).</p>"},{"location":"4-chapter-2-solutions/#q2-what-happens-if-we-do-not-specify-how-many-lambdas-we-want","title":"Q2: what happens if we do not specify how many 's we want?","text":"<p>If we remove the lambda options: <pre><code>set.seed(2)\nX &lt;- matrix(rnorm(100 * 20), 100, 20)\nbeta &lt;- c(rep(5,5),rep(0,15))\ny &lt;- X%*%beta + rnorm(100)\n\nlibrary(glmnet)\nfit &lt;- glmnet(x = X, y = y, family = \"gaussian\")\nlength(fit$lambda)\n</code></pre> We see that we are now using 65  values. The default value for the function is actually 100, but the function will also end the path early if the deviance ratio (<code>fit$dev.ratio</code>) is close to 1, which happened here.</p>"},{"location":"4-chapter-2-solutions/#q3-optional-the-glmnet-documentation-states-that-there-is-a-parameter-alpha-that-we-can-change-what-does-this-do","title":"Q3 (optional): the glmnet documentation states that there is a parameter  that we can change. What does this do?**","text":"<p>The <code>glmnet</code> function actually runs the elastic net model, which is defined by  It uses a combination of the lasso and ridge (similar to how SGL combines the lasso and group lasso), balanced through the  parameter. By default,  is set to 1, so that it reduces to the lasso, which is why we have not had to worry about it so far. However, elastic net has been proposed as an extension to the lasso which overcomes many of its issues, so we compare their performances here. <pre><code>set.seed(2)\nX &lt;- matrix(rnorm(100 * 20), 100, 20)\nbeta &lt;- c(rep(5,5),rep(0,15))\ny &lt;- X%*%beta + rnorm(100)\n\nset.seed(3)\nX_new &lt;- matrix(rnorm(10 * 20), 10, 20)\ny_new &lt;- X_new%*%beta + rnorm(10)\n\nlibrary(glmnet)\nalpha_seq = seq(0,1,length.out=20)\npreds = data.frame(alpha = alpha_seq, error = rep(0,length(alpha_seq)))\nfor (i in 1:length(alpha_seq)){\n    set.seed(2)\n    fit.cv &lt;- cv.glmnet(x = X, y = y, family = \"gaussian\", nlambda = 20, lambda.min.ratio = 0.1, alpha = alpha_seq[i])\n    preds$error[i] = mean((y_new-predict(object = fit.cv, newx = X_new, s = \"lambda.1se\"))^2)\n}\nplot(preds$alpha, preds$error, type = \"b\", ylab=\"Error\")\n</code></pre></p> <p></p> <p>Clearly the model performs badly for , which is ridge regression. This is not surprising, as ridge regression does not shrink coefficients exactly to zero, so if we take a close look at the coefficients, we notice that it is forced to keep coefficients active <pre><code>set.seed(2)\nfit.cv.ridge &lt;- cv.glmnet(x = X, y = y, family = \"gaussian\", nlambda = 20, lambda.min.ratio = 0.1, alpha = 0)\ncbind(beta, fit.cv.ridge$glmnet.fit$beta[,20])\n</code></pre> Looking at the predictions in more detail <pre><code>preds\n</code></pre> We can see that  achieves the lowest error (which is similar to the recommended  value for SGL). Comparing the coefficients it is clear to see why this value works (as we are able to make inactive coefficients exactly zero) <pre><code>set.seed(2)\nfit.cv.best &lt;- cv.glmnet(x = X, y = y, family = \"gaussian\", nlambda = 20, lambda.min.ratio = 0.1, alpha = 0.94736842)\ncbind(beta, fit.cv.ridge$glmnet.fit$beta[,20], fit.cv.best$glmnet.fit$beta[,20])\n</code></pre></p>"},{"location":"4-chapter-2-solutions/#q4-optional-look-at-the-glmnet-documentation-which-parameters-might-be-interesting-to-vary","title":"Q4 (optional): look at the glmnet documentation - which parameters might be interesting to vary?","text":"<p>There are many options to alter in the <code>glmnet</code> function. Generally, it is best to leave them as default, as they have been set to sensible values by the authors, unless you have a reason to change them. Some ones of interest are:</p> <ul> <li><code>alpha</code>: this is discussed in Q3.</li> <li><code>standardize</code>: a TRUE/FALSE indicator as to whether the data is standardised. It is good practice to standardise data, so this should be left on. It is not a good idea to standardise the data yourself and then feed this to <code>glmnet</code>, because standardisation alters how  is used (it scales  in the backend, see Q1).</li> <li><code>intercept</code>: a TRUE/FALSE indicator as to whether an intercept is fit. Again, it is good practice to leave this on, unless you have a strong reason to believe that your regression line goes through the origin (i.e., that your response is centered at 0, which is rare). You also should not center your response yourself, as again various changes occur in the backend if this is set to on. The two options described show that you do not need to do these pre-processing steps yourself, <code>glmnet</code> will do it for you.</li> <li><code>penalty.factor</code>: this allows you to use adaptive penalty weights, which leads to the adaptive lasso (as in SLOPE, see the optional section, although note that this function can not be used for SLOPE, due to the sorting component of SLOPE). </li> </ul>"},{"location":"4-chapter-2-solutions/#q5-instead-of-using-the-predict-function-manually-code-a-prediction","title":"Q5: instead of using the predict function, manually code a prediction.","text":"<p>A linear model is defined as  so, to form a prediction, we need to plug in our values of  values that we found in the model. We can do that using <pre><code>set.seed(2)\nX &lt;- matrix(rnorm(100 * 20), 100, 20)\nbeta &lt;- c(rep(5,5),rep(0,15))\ny &lt;- X%*%beta + rnorm(100)\n\nlibrary(glmnet)\nfit &lt;- glmnet(x = X, y = y, family = \"gaussian\", lambda = 0.5, intercept = FALSE)\n\npreds_1 = predict(object = fit, newx = X)\npreds_2 = X%*%fit$beta\ncbind(preds_1[1:5], preds_2[1:5])\n</code></pre></p>"},{"location":"4-chapter-2-solutions/#q6-compare-the-predictions-of-the-1se-model-against-the-min-model","title":"Q6: compare the predictions of the 1se model against the min model.","text":"<p><pre><code>set.seed(2)\nX &lt;- matrix(rnorm(100 * 20), 100, 20)\nbeta &lt;- c(rep(5,5),rep(0,15))\ny &lt;- X%*%beta + rnorm(100)\n\nset.seed(3)\nX_new &lt;- matrix(rnorm(10 * 20), 10, 20)\ny_new &lt;- X_new%*%beta + rnorm(10)\n\nlibrary(glmnet)\nfit.cv &lt;- cv.glmnet(x = X, y = y, family = \"gaussian\", nlambda = 20, lambda.min.ratio = 0.1)\nmean((y_new-predict(object = fit.cv, newx = X_new, s = \"lambda.1se\"))^2)\nmean((y_new-predict(object = fit.cv, newx = X_new, s = \"lambda.min\"))^2)\n</code></pre> In this case, the minimum value actually obtains a lower predictive error, but generally it is still recommended to use the 1se model to reduce variance (overfitting).</p>"},{"location":"4-chapter-2-solutions/#q7-compare-the-grplasso-package-to-glmnet-to-see-if-standardisation-works-properly","title":"Q7: compare the <code>grplasso</code> package to <code>glmnet</code> to see if standardisation works properly?","text":"<p>To do this, we need to reduce the group lasso to the lasso. We can use singleton groups (each variable in its own group), so that the two models are equivalent. We have set the <code>grplasso</code> model up to use the in-built standardisation  <pre><code>set.seed(2)\nX &lt;- matrix(rnorm(100 * 20), 100, 20)\nbeta &lt;- c(rep(5,5),rep(0,15))\ny &lt;- X%*%beta + rnorm(100)\n\nlibrary(glmnet)\nlibrary(grplasso)\nlasso_model &lt;- glmnet(x = X, y = y, family = \"gaussian\", nlambda = 20, lambda.min.ratio = 0.1)\nglasso_model &lt;-  grplasso(x = cbind(1,X), y = y, index = c(NA,1:ncol(X)), lambda = lasso_model$lambda, standardize = TRUE, center = TRUE, model = LinReg())\n</code></pre></p> <p>Comparing the final  solution, we see that they are not the same, so the built-in standardisation is not working as intended. <pre><code>cbind(lasso_model$beta[,20], glasso_model$coefficients[-1,20])\n</code></pre></p>"},{"location":"4-chapter-2-solutions/#q8-set-the-group-indexing-so-that-the-signal-variables-are-all-in-the-same-group-what-do-you-observe","title":"Q8: set the group indexing so that the signal variables are all in the same group. What do you observe?","text":"<p>Choosing a different grouping, so that the signal variables are in the same group, would lead to the following <pre><code>set.seed(2)\nX &lt;- matrix(rnorm(100 * 20), 100, 20)\nbeta &lt;- c(rep(5,5),rep(0,15))\ny &lt;- X%*%beta + rnorm(100)\ngroups &lt;- c(rep(1,4),rep(2,4),rep(3,4),rep(4,4),rep(5,4))\ngroups_gl &lt;- c(NA, groups) # we add NA as the intercept does not belong to a group\n\n# generate lambda sequence\nlambda_max_group &lt;- lambdamax(X_gl, y, groups_gl, standardize = FALSE, center = FALSE) # finding lambda max, we can ignore the warning\nlambdas_gl &lt;- exp(seq( # calculating the full lambda sequence\n  from = log(lambda_max_group),\n  to = log(lambda_max_group * 0.1), # the equivalent of lambda.min.ratio in glmnet\n  length.out = 20 # how many lambdas we want\n))\n\n# fit model\nglasso_model &lt;- grplasso(x = X_gl, y = y, index = groups_gl, lambda = lambdas_gl, standardize = FALSE, center = FALSE, model = LinReg())\n\ncbind(beta,glasso_model$coefficients[-1,20])\n\nmean((y_new-predict(object = glasso_model, newdata = cbind(1,X_new))[,20])^2)\n</code></pre> We now see that we are no longer selecting a lot of zero variables, although surprisingly, this actually makes the prediction error larger. </p>"},{"location":"4-chapter-2-solutions/#q9-optional-can-you-figure-out-why-we-get-inflated-values","title":"Q9 (optional): can you figure out why we get inflated values?","text":"<p>If we turn standardisation off we get: <pre><code>set.seed(2)\nX &lt;- matrix(rnorm(100 * 20), 100, 20)\nbeta &lt;- c(rep(5,5),rep(0,15))\ny &lt;- X%*%beta + rnorm(100)\nlibrary(SGL)\nsgl_model &lt;- SGL(list(x=X,y=y), groups, type = \"linear\", nlam = 20, min.frac = 0.1, alpha = 0.95)\nsgl_model_2 &lt;- SGL(list(x=X,y=y), groups, type = \"linear\", nlam = 20, min.frac = 0.1, alpha = 0.95, standardize = FALSE)\n\ncbind(beta, sgl_model$beta[,20], sgl_model_2$beta[,20])\n</code></pre> So it appears that standardisation is not properly implemented in the <code>SGL</code> package. This highlights the issue of pre-processing when it is done incorrectly.</p>"},{"location":"4-chapter-2-solutions/#q10-what-happens-to-the-predictive-score-if-we-allow-lambda-to-decrease-even-further","title":"Q10: what happens to the predictive score if we allow  to decrease even further?","text":"<p>We can test the predictive score by decreasing  to quite an extreme minimum and allowing for more  values along the path: <pre><code>set.seed(2)\nX &lt;- matrix(rnorm(100 * 20), 100, 20)\nbeta &lt;- c(rep(5,5),rep(0,15))\ny &lt;- X%*%beta + rnorm(100)\n\nset.seed(3)\nX_new &lt;- matrix(rnorm(10 * 20), 10, 20)\ny_new &lt;- X_new%*%beta + rnorm(10)\n\nlibrary(SGL)\nsgl_model &lt;- SGL(list(x=X,y=y), groups, type = \"linear\", nlam = 200, min.frac = 0.001, alpha = 0.95)\n\npreds = rep(0,200)\nfor (i in 1:200){\n    preds[i] = mean((y_new-predictSGL(x = sgl_model, newX = X_new, lam = i))^2)\n}\nplot(preds,type=\"l\",ylab=\"Error\")\n</code></pre></p> <p></p> <p>We see that after a certain point, decreasing  further does not provide any additional benefit, only adding more model complexity. Generally, we prefer to use the simplest model that is available, without sacrificing accuracy (a concept known as Occam's Razor).</p> <p>We can try an even smaller value of <code>min.frac</code> <pre><code>sgl_model &lt;- SGL(list(x=X,y=y), groups, type = \"linear\", nlam = 200, min.frac = 0.000001, alpha = 0.95)\nmean((y_new-predictSGL(x = sgl_model, newX = X_new, lam = 20))^2)\n\npreds = rep(0,20)\nfor (i in 1:20){\n    preds[i] = mean((y_new-predictSGL(x = sgl_model, newX = X_new, lam = i))^2)\n}\nplot(preds,type=\"b\",ylab=\"Error\")\n</code></pre></p> <p></p> <p>The prediction error continues to decrease but we are adding a lot of variance into the model by overfitting. This is ok for a simple example like this, but when there are more predictors it can become problematic, especially if the divide between signal and noise is less apparant.</p>"},{"location":"4-chapter-2-solutions/#q11-vary-alpha-in-the-region-01-what-do-you-observe","title":"Q11: vary  in the region . What do you observe?","text":"<pre><code>set.seed(2)\nX &lt;- matrix(rnorm(100 * 20), 100, 20)\nbeta &lt;- c(rep(5,5),rep(0,15))\ny &lt;- X%*%beta + rnorm(100)\n\nset.seed(3)\nX_new &lt;- matrix(rnorm(10 * 20), 10, 20)\ny_new &lt;- X_new%*%beta + rnorm(10)\nlibrary(SGL)\n\nalpha_seq = seq(0,1,length.out=20)\npreds = data.frame(alpha = alpha_seq, error = rep(0,length(alpha_seq)))\nfor (i in 1:length(alpha_seq)){\n    set.seed(2)\n    fit &lt;- SGL(list(x=X,y=y), groups, type = \"linear\", nlam = 20, min.frac = 0.001, alpha = alpha_seq[i])\n    preds$error[i] = mean((y_new-predictSGL(x = fit, newX = X_new, lam = 20))^2)\n}\nplot(preds$alpha, preds$error, type = \"b\", ylab=\"Error\",xlab=\"Alpha\")\n</code></pre> <p>We observe the error shrinking as  gets close to 1 (the lasso). This is expected in this scenario, as we did not add a grouping structure to the synthetic data.</p>"},{"location":"4-chapter-2-solutions/#q12-can-you-use-the-sgl-r-package-to-fit-the-lasso-and-group-lasso","title":"Q12: can you use the <code>SGL</code> R package to fit the lasso and group lasso?","text":"<p>To do this, we just set <code>alpha</code> to 0 and 1: <pre><code>set.seed(2)\nX &lt;- matrix(rnorm(100 * 20), 100, 20)\nbeta &lt;- c(rep(5,5),rep(0,15))\ny &lt;- X%*%beta + rnorm(100)\n\nlibrary(SGL)\nfit_glasso &lt;- SGL(list(x=X,y=y), groups, type = \"linear\", nlam = 20, min.frac = 0.001, alpha = 0)\nfit_lasso &lt;- SGL(list(x=X,y=y), groups, type = \"linear\", nlam = 20, min.frac = 0.001, alpha = 1)\n</code></pre> Even though this will give results very similar to the <code>glmnet</code> and <code>grplasso</code> packages, it is still recommended to use the package specific to the model, as it will be more optimised.</p>"},{"location":"5-chapter-3/","title":"Chapter 3: Predicting diseases application","text":"<p>We are finally ready to carry out the final objective: using penalised regression to predict diseases. </p>"},{"location":"5-chapter-3/#load-data","title":"Load data","text":"<p>The first task is to load in the data we prepared in Chapter 1. <pre><code>setwd(\"data\")\ndata = readRDS(\"colitis-data-c3.RDS\")\nX = data$X\ny = data$y\ngroups = data$groups\nrm(data)\n</code></pre></p> <p>We want to test the models out by fitting on training data and evaluating them on test data. Therefore, we will split the data randomly into train/test datasets. The split proportion between the two datasets is a subjective choice, but we want there to be enough data to fit an informative model, but also have enough test data to conduct an extensive validation. Commonly used proportions of train/test are anything from 50/50 to 80/20. In this case, we use approximately 60/40.</p> <pre><code>set.seed(100)\ntraining_ind &lt;- sample(1:nrow(X), 50) \ntrain_data &lt;- list(X = X[training_ind,], y = y[training_ind])\ntest_data &lt;- list(X = X[-training_ind,], y = y[-training_ind])\n</code></pre>"},{"location":"5-chapter-3/#initial-hyperparameters","title":"Initial hyperparameters","text":"<p>To fit the various models, we need to define the hyperparameters so that we are consistent across the models, allowing for fairer comparison. The parameters that we need to decide upon are (some of these were explained in Chapter 2):</p> <ul> <li><code>path_length</code>: this defines how many values of  we will fit the model for. We will set this to  to allow for more models to be fit. Ideally we want this to be as large as possible, to give as many possible models, but we must also think about computational cost. </li> <li><code>min_frac</code>: this sets the value that the final  value is set to, in the sense that . Making this small means that we will allow denser models to be fit. We set this to  to allow denser models to be considered.</li> <li><code>alpha</code>: this is only used for SGL (and SGS in the optional questions): this defines the balance between the variable and group penalties. In genetics, we usually encounter large groups, with many noisy variables. As such, we would prefer to not be limited by the group penalties, in which full groups are picked, but we still want to utilize grouping information. As such, we will use  (which is a slight deviation from the recommended  value discussed in Chapter 2, showing that sometimes a problem requires deviation from the recommended usage of a model).</li> <li><code>num_iter</code>: this is the maximum number of iterations the fitting algorithm should fit for, if convergence is not reached. This tends to be set at , but as the dataset is quite large, we allow the fitting algorithms to run for longer.</li> </ul> <p>We will also use  standardisation for each model and fit an intercept.</p> <pre><code>path_length = 100\nmin_frac = 0.01\nalpha = 0.99\nnum_iter = 10000\n</code></pre>"},{"location":"5-chapter-3/#fit-lasso-model","title":"Fit lasso model","text":"<p>Now that the data is loaded, we can fit a lasso model. Note that this time we have set <code>family=\"binomial\"</code> as our response  is binary.  <pre><code>library(glmnet)\nlasso_model &lt;- glmnet(\n  x = train_data$X,\n  y = train_data$y,\n  family = \"binomial\",\n  lambda.min.ratio = min_frac,\n  maxit = num_iter,\n  standardize = TRUE,\n  intercept = TRUE\n) \n</code></pre> We can investigate the fitted values <pre><code>plot(lasso_model)\n</code></pre></p> <p></p> <p>and also see how many variables are entering the model as we decrease : <pre><code>plot(apply(lasso_model$beta, 2, function(x) length(which(x!=0))), type=\"l\", xlab=\"Lambda index\", ylab = \"Number non-zero\")\n</code></pre></p> <p></p> <p>At the most saturated point, we have about  genes in the model.</p>"},{"location":"5-chapter-3/#prediction","title":"Prediction","text":"<p>Before we move onto the next models, let's see how well the lasso performs in prediction. </p> <p>First, we use <code>predict</code> function on the lasso model. <pre><code># calculate predictions\nlasso_preds &lt;- predict(lasso_model, test_data$X, type = \"class\")\nlasso_preds[1:5,1:5] # a snapshot\n</code></pre></p> <p>Next, we need to compare the predictions to the test data and output this into a new dataframe. To do this, we are checking for each  index (each column of the prediction matrix) how many times on average the prediction matches the test data (which tells us the classification accuracy).</p> <p><pre><code># compare to test data\nlasso_cr &lt;- apply(lasso_preds, 2, function(x) mean(x == test_data$y))\n\n# put classification scores into data frame\nlasso_df = data.frame(\n  model = \"Lasso\",\n  lambda_index = 1:path_length,\n  classification_rate = lasso_cr\n)\n</code></pre> We can now view the classification rate as a function of the  index. <pre><code>plot(x = lasso_df$lambda_index, y = lasso_df$classification_rate, type=\"l\", xlab=\"Lambda index\", ylab = \"Classification accuracy\")\nabline(v = which.max(lasso_df$classification_rate), col = \"red\") # where the maximum is located\n</code></pre></p> <p></p> <p><pre><code>max(lasso_df$classification_rate)\nwhich.max(lasso_df$classification_rate)\napply(lasso_model$beta, 2, function(x) length(which(x!=0)))[which.max(lasso_df$classification_rate)]\n</code></pre> So the best model appears to be the one at the  index of , achieving a peak classification score of . Looking back again at the plot showing the number of genes, we see that the best model is not the one using the most amount of genes, but in fact needs only  genes</p> <pre><code>plot(apply(lasso_model$beta, 2, function(x) length(which(x!=0))), type=\"l\", xlab=\"Lambda index\", ylab = \"Number non-zero\")\nabline(v = which.max(lasso_df$classification_rate), col = \"red\")\n</code></pre> <p></p> <p>To gain further insight into the best lasso predictive model, we can plot the decision boundaries. Decision boundaries in classification problems show how close each data point was to being classified differently (we are plotting the class probabilities).</p> <pre><code>lasso_class_prob &lt;- predict(lasso_model, test_data$X, type = \"response\")[,which.max(lasso_df$classification_rate)]\nfalse_pred = which(!ifelse(lasso_class_prob&gt;=0.5,1,0) == test_data$y)\nplot(lasso_class_prob,pch=16, ylab = \"Class probability\",ylim=c(0,1))\nabline(h=0.5)\npoints(test_data$y,col=ifelse(test_data$y==\"TRUE\",\"forestgreen\",\"red\"),pch=4)\npoints(x = false_pred, y = lasso_class_prob[false_pred], col = \"orange\", pch = 16)\n</code></pre> <p></p> <p>The coloured crosses represent the true class of the observations. The lasso appears fairly confident on most observations, not choosing too many probabilities close to the decision boundary of . The orange points highlight the incorrectly identified observations. The cluster of three on the right are close to the boundary, but the other two on the left are not.</p> <p>Q1: what happens when you don't fit an intercept? What about no standardisation?</p> <p>Q2: apply the lasso to the cancer data</p> <p>Q3 (optional): <code>glmnet</code> has the elastic net model. Apply it to the colitis data.</p>"},{"location":"5-chapter-3/#group-lasso-glasso","title":"Group lasso (gLasso)","text":"<p>We move on to fitting the group lasso model. As mentioned in Chapter 2, the <code>grplasso</code> package does not perform standardisation properly, so we do this ourselves before fitting. </p> <pre><code>library(grplasso)\nX_gl &lt;- t(t(train_data$X) - apply(train_data$X, 2, mean))\nX_gl &lt;- t(t(X_gl) / apply(X_gl, 2, sd))\nX_gl &lt;- cbind(1, X_gl)\ngroups_gl &lt;- c(NA, groups)\n\nlambda_max_group &lt;- lambdamax(X_gl, as.numeric(train_data$y), groups_gl, standardize = FALSE)\nlambdas_gl &lt;- exp(seq(\n  from = log(lambda_max_group),\n  to = log(lambda_max_group * min_frac),\n  length.out = path_length\n))\nglasso_model &lt;- grplasso(\n  x = X_gl,\n  y = as.numeric(train_data$y),\n  index = groups_gl,\n  lambda = lambdas_gl,\n  standardize = FALSE,\n  max.iter = num_iter\n)\n</code></pre> <p>As before, we investigate the solution <pre><code>plot(glasso_model)\n</code></pre></p> <p></p> <p>and also see how many variables are entering the model as we decrease : <pre><code>plot(apply(glasso_model$coefficients, 2, function(x) length(which(x!=0))), type=\"l\", xlab=\"Lambda index\", ylab = \"Number non-zero\")\n</code></pre></p> <p></p> <p>At the most saturated point, we have  genes in the model.</p>"},{"location":"5-chapter-3/#prediction_1","title":"Prediction","text":"<p>We extract the predictions for gLasso <pre><code>glasso_preds &lt;- predict(object = glasso_model, newdata = cbind(1,test_data$X), type = \"response\")\nglasso_preds[1:5,1:5]\n</code></pre> However, we notice that unlike the <code>glmnet</code> function, the predictions are not given in their final binary format, but are instead probabilities, so we need to convert these. This is another example of a package not having all the features that <code>glmnet</code> has. <pre><code>glasso_preds = ifelse(glasso_preds &gt;= 0.5, 1, 0)\nglasso_cr &lt;- apply(glasso_preds, 2, function(x) mean(x == test_data$y))\n\n# put classification scores into data frame\nglasso_df = data.frame(\n  model = \"gLasso\",\n  lambda_index = 1:path_length,\n  classification_rate = glasso_cr\n)\n</code></pre> We now perform the same visualisations as for the lasso. <pre><code>plot(x = glasso_df$lambda_index, y = glasso_df$classification_rate, type=\"l\", xlab=\"Lambda index\", ylab = \"Classification accuracy\")\nabline(v = which.max(glasso_df$classification_rate), col = \"red\") # where the maximum is located\n</code></pre></p> <p></p> <p><pre><code>max(glasso_df$classification_rate)\nlength(which(glasso_model$coefficients[,which.max(glasso_df$classification_rate)]!=0))\n</code></pre> <pre><code>plot(apply(glasso_model$coefficients, 2, function(x) length(which(x!=0))), type=\"l\", xlab=\"Lambda index\", ylab = \"Number non-zero\")\nabline(v = which.max(glasso_df$classification_rate), col = \"red\")\n</code></pre></p> <p></p> <p>The best model appears to be the one at the  index of , achieving a peak classification score of  using  genes. This is another example highlighting the downside of applying only groupwise sparsity. By being forced to pick all variables in a group as active, we are using a lot of noise variables to form our prediction, leading to a decrease in the classification accuracy of  in comparison to the lasso. The added complexity of applying a group penalty does not yield any benefit over the simpler lasso. We now turn to SGL to see if this can resolve some of these issues.</p> <p>Q4: apply the group lasso to the colitis data.</p>"},{"location":"5-chapter-3/#sparse-group-lasso-sgl","title":"Sparse-group lasso (SGL)","text":"<p>The <code>SGL</code> package crashes R when applied to the colitis dataset. This appears to be a bug in the package. Instead, we will use the <code>sgs</code> package, which fits SGS models. SGS models can be reduced to SGL models by using constant weights (as indicated by the choice of <code>v_weights</code> and <code>w_weights</code> below). <pre><code>library(sgs)\nsgl_model = fit_sgs(\n  X = train_data$X,\n  y = train_data$y,\n  groups = groups,\n  type = \"logistic\",\n  path_length = path_length,\n  min_frac = min_frac,\n  alpha = alpha,\n  max_iter = num_iter,\n  screen = TRUE,\n  intercept = FALSE,\n  verbose = TRUE,\n  v_weights = rep(1, ncol(train_data$X)),\n  w_weights = rep(1, length(unique(groups)))\n)\n</code></pre> Performing the predictiton: <pre><code>sgl_preds = predict(sgl_model, x = test_data$X)\nsgl_cr &lt;- apply(sgl_preds$class, 2, function(x) mean(x == test_data$y))\n\n# put classification scores into data frame\nsgl_df = data.frame(\n  model = \"SGL\",\n  lambda_index = 1:path_length,\n  classification_rate = sgl_cr\n)\nmax(sgl_df$classification_rate)\nwhich.max(sgl_df$classification_rate)\n</code></pre> SGL obtains a peak accuracy of  at the index of , using  genes: <pre><code>length(sgl_model$selected_var[[which.max(sgl_df$classification_rate)]])\n</code></pre> We observe that SGL outperforms the lasso and group lasso, highlighting the benefit of applying sparse-group penalisation. The group lasso is limited as it is forced to make every gene within an active group active, leading to a lot of noise being included in the model. On the other hand, the lasso does not utilise grouping information. SGL is able to overcome both of these issues.</p> <p>Q5: apply SGL to the colitis data.</p>"},{"location":"5-chapter-3/#comparison-of-models","title":"Comparison of models","text":""},{"location":"5-chapter-3/#number-of-non-zero-coefficients","title":"Number of non-zero coefficients","text":"<pre><code>plot(apply(lasso_model$beta, 2, function(x) length(which(x!=0))), type=\"l\", xlab=\"Lambda index\", ylab = \"Number non-zero\", ylim = c(0,40))\nlines(apply(glasso_model$coefficients, 2, function(x) length(which(x!=0))), type=\"l\", col = \"red\")\nlines(unlist(lapply(sgl_model$selected_var,length)), type = \"l\", col = \"brown\")\nlegend(\"topright\", legend = c(\"Lasso\", \"gLasso\", \"SGL\"), \n       col = c(\"black\", \"red\", \"brown\"), lty = 1, cex = 0.8)\n</code></pre>"},{"location":"5-chapter-3/#prediction-accuracies","title":"Prediction accuracies","text":"<pre><code>plot(lasso_df$classification_rate, type=\"l\", xlab=\"Lambda index\", ylab = \"Number non-zero\")\nlines(glasso_df$classification_rate, type=\"l\", col = \"red\")\nlines(sgl_df$classification_rate, type = \"l\", col = \"brown\")\nlegend(\"topright\", legend = c(\"Lasso\", \"gLasso\", \"SGL\"), \n       col = c(\"black\", \"red\", \"brown\"), lty = 1, cex = 0.8)\n</code></pre> Model Classification accuracy (%) Genes used Lasso 93.5 11 gLasso 89.6 23 SGL 94.8 25"},{"location":"5-chapter-3/#slope-models-optional","title":"SLOPE models (optional)","text":"<p>We covered SLOPE models in the optional section of Chapter 2. Here, we apply them to the genetics dataset to get some insight into whether the adaptive and sorting components lead to an improvement over the lasso. We apply three different SLOPE-based models, which will allow for comparison to their lasso equivalence.</p>"},{"location":"5-chapter-3/#slope","title":"SLOPE","text":"<p>The SLOPE package has a lot of options that we can configure. They have been configured here to allow for direct comparison to the other models, ensuring that we also use  different  values. The key quantity that we have had to define compared to the previous models is the <code>q</code> parameter, which corresponds to the desired FDR level. In this case, due to the large size of the dataset, we set this to be a very low value to ensure that a strong level of penalisation is applied. </p> <p><pre><code>library(SLOPE)\n\nslope_model = SLOPE(\n  x = train_data$X,\n  y = train_data$y,\n  q = 1e-4,\n  family = \"binomial\",\n  intercept = TRUE,\n  scale = \"l2\",\n  center = TRUE,\n  alpha = \"path\",\n  lambda = \"bh\",\n  alpha_min_ratio = min_frac,\n  max_passes = 10000,\n  path_length = path_length,\n  tol_dev_ratio = 1000000,\n  max_variables = 10000,\n  tol_dev_change = 0\n)\n</code></pre> We now use the SLOPE model to form a prediction. <pre><code>slope_preds = predict(slope_model, x = test_data$X, type = \"response\") \nslope_preds = ifelse(slope_preds &gt;= 0.5, 1, 0)\nslope_cr &lt;- apply(slope_preds, 2, function(x) mean(x == test_data$y))\n\n# put classification scores into data frame\nslope_df = data.frame(\n  model = \"SLOPE\",\n  lambda_index = 1:path_length,\n  classification_rate = slope_cr\n)\nmax(slope_df$classification_rate)\nwhich.max(slope_df$classification_rate)\n</code></pre> SLOPE is found to have a peak classification rate of  at index . </p> <p><pre><code>max(slope_df$classification_rate)\nwhich.max(slope_df$classification_rate)\nsum(slope_model$nonzeros[,,which.max(slope_df$classification_rate)])\n</code></pre> This model uses  genes.</p> <p>Q6 (optional): apply SLOPE to the colitis data.</p>"},{"location":"5-chapter-3/#group-slope-gslope","title":"Group SLOPE (gSLOPE)","text":"<p>To fit a gSLOPE model, in this seciton we will use the <code>sgs</code> package instead of the <code>grpSLOPE</code> package, as in Chapter 2. The <code>sgs</code> package has a function for fitting gSLOPE models which contains useful features that the <code>grpSLOPE</code> package does not have.</p> <p>The parameters are as for the other models. The <code>lambda</code> sequence option is set to <code>mean</code>, which is a sequence derived for gSLOPE to control the false-discovery rate, with the corresponding parameter for this sequence set to <code>1e-4</code>. We turn the intercept off here, as the <code>sgs</code> package has a bug when using logistic models and intercepts, but given the high dimensionality, the intercept is not particularly important anyway. The key feature for the <code>sgs</code> package is that it performs screening, which is an approach that massively speeds up model fitting, particularly in settings where we have many features.</p> <p><pre><code>library(sgs)\ngslope_model = fit_gslope(\n  X = train_data$X,\n  y = train_data$y,\n  groups = groups,\n  type = \"logistic\",\n  path_length = path_length,\n  min_frac = min_frac,\n  gFDR = 1e-4, \n  max_iter = num_iter,\n  screen = TRUE,\n  intercept = FALSE, \n  verbose = TRUE\n)\n</code></pre> Performing the predictiton: <pre><code>gslope_preds = predict(gslope_model, x = test_data$X)\ngslope_cr &lt;- apply(gslope_preds$class, 2, function(x) mean(x == test_data$y))\n\n# put classification scores into data frame\ngslope_df = data.frame(\n  model = \"gSLOPE\",\n  lambda_index = 1:path_length,\n  classification_rate = gslope_cr\n)\nmax(gslope_df$classification_rate)\nwhich.max(gslope_df$classification_rate)\n</code></pre> gSLOPE obtains a peak accuracy of  at the index of 69. <pre><code>length(gslope_model$selected_var[[which.max(gslope_df$classification_rate)]])\n</code></pre> And this model uses  genes, which is over twice the amount that SLOPE uses. This is another illustration of the downside of applying just a group penalty. Not only is the prediction accuracy lower, but the model also uses more genes. </p> <p>Q7 (optional): apply gSLOPE to the colitis data.</p>"},{"location":"5-chapter-3/#sparse-group-slope-sgs","title":"Sparse-group SLOPE (SGS)","text":"<p>The final model we will test is sparse-group SLOPE (SGS), which applies adaptive penalisation at both the variable and group levels. In theory, this model should apply the strongest amount of penalisation, leading to the most sparse models. We can use the <code>sgs</code> package, as discussed in Chapter 2. SGS has three different choices of penalty sequences. Here, we have set <code>pen_method = 3</code>, as this is the fastest (computationally) sequence to calculate.</p> <p><pre><code>library(sgs)\nsgs_model = fit_sgs(\n  X = train_data$X,\n  y = train_data$y,\n  groups = groups,\n  type = \"logistic\",\n  path_length = path_length,\n  min_frac = min_frac,\n  gFDR = 1e-4,\n  vFDR = 1e-4, \n  alpha = alpha,\n  max_iter = num_iter,\n  screen = TRUE,\n  intercept = FALSE,\n  verbose = TRUE,\n  pen_method = 3  \n)\n</code></pre> Performing the predictiton: <pre><code>sgs_preds = predict(sgs_model, x = test_data$X)\nsgs_cr &lt;- apply(sgs_preds$class, 2, function(x) mean(x == test_data$y))\n\n# put classification scores into data frame\nsgs_df = data.frame(\n  model = \"SGS\",\n  lambda_index = 1:path_length,\n  classification_rate = sgs_cr\n)\nmax(sgs_df$classification_rate)\nwhich.max(sgs_df$classification_rate)\n</code></pre> SGS obtains a peak accuracy of  at the index of , using  genes: <pre><code>length(sgs_model$selected_var[[which.max(sgs_df$classification_rate)]])\n</code></pre></p> <p>Q8 (optional): can you achieve a higher predictive accuracy with SGS?</p> <p>Q9 (optional): apply SGS to the colitis data.</p>"},{"location":"5-chapter-3/#number-of-non-zero-coefficients_1","title":"Number of non-zero coefficients","text":"<pre><code>plot(apply(lasso_model$beta, 2, function(x) length(which(x!=0))), type=\"l\", xlab=\"Lambda index\", ylab = \"Number non-zero\", ylim=c(0,60))\nlines(apply(glasso_model$coefficients, 2, function(x) length(which(x!=0))), type=\"l\", col = \"red\")\nlines(unlist(lapply(sgl_model$selected_var,length)), type = \"l\", col = \"brown\")\nlines(apply(slope_model$nonzeros,3,sum), type = \"l\", col = \"blue\")\nlines(unlist(lapply(gslope_model$selected_var,length)), type = \"l\", col = \"green\")\nlines(unlist(lapply(sgs_model$selected_var,length)), type = \"l\", col = \"purple\")\nlegend(\"topleft\", legend = c(\"Lasso\", \"gLasso\", \"SGL\", \"SLOPE\", \"gSLOPE\", \"SGS\"),\n       col = c(\"black\", \"red\", \"brown\", \"blue\", \"green\", \"purple\"), lty = 1, )\n</code></pre>"},{"location":"5-chapter-3/#prediction-accuracies_1","title":"Prediction accuracies","text":"<pre><code>plot(lasso_df$classification_rate, type=\"l\", xlab=\"Lambda index\", ylab = \"Classification accuracy\", ylim=c(0.3,1))\nlines(glasso_df$classification_rate, type=\"l\", col = \"red\")\nlines(sgl_df$classification_rate, type = \"l\", col = \"brown\")\nlines(slope_df$classification_rate, type = \"l\", col = \"blue\")\nlines(gslope_df$classification_rate, type = \"l\", col = \"green\")\nlines(sgs_df$classification_rate, type = \"l\", col = \"purple\")\nlegend(\"bottomright\", legend = c(\"Lasso\", \"gLasso\", \"SGL\", \"SLOPE\", \"gSLOPE\", \"SGS\"),\n       col = c(\"black\", \"red\", \"brown\", \"blue\", \"green\", \"purple\"), lty = 1)\n</code></pre> Model Classification accuracy (%) Genes used Lasso 93.5 11 gLasso 89.6 23 SGL 94.8 25 SLOPE 94.8 16 gSLOPE 80.5 43 SGS 94.8 23"},{"location":"6-chapter-3-solutions/","title":"Chapter 3 solutions","text":""},{"location":"6-chapter-3-solutions/#q1-what-happens-when-you-dont-fit-an-intercept-what-about-no-standardisation","title":"Q1: what happens when you don't fit an intercept? What about no standardisation?","text":"<p>We first turn off the intercept option. <pre><code>library(glmnet)\nlasso_model_no_intercept &lt;- glmnet(\n  x = train_data$X,\n  y = train_data$y,\n  family = \"binomial\",\n  lambda.min.ratio = min_frac,\n  maxit = num_iter,\n  standardize = TRUE,\n  intercept = FALSE\n) \n</code></pre> and we obtain an error to say that convergence for the 99th lambda value was not reached. This is because applying an intercept also centers the data matrix, which helps in the fitting process. Now, let's try removing standardisation. <pre><code>lasso_model_no_sd &lt;- glmnet(\n  x = train_data$X,\n  y = train_data$y,\n  family = \"binomial\",\n  lambda.min.ratio = min_frac,\n  maxit = num_iter,\n  standardize = TRUE,\n  intercept = FALSE\n) \n</code></pre> No error this time, so let's see how it performs for prediction. <pre><code>lasso_preds_no_sd &lt;- predict(lasso_model_no_sd, test_data$X, type = \"class\")\nlasso_cr_no_sd &lt;- apply(lasso_preds_no_sd, 2, function(x) mean(x == test_data$y))\n\n# put classification scores into data frame\nlasso_df_no_sd = data.frame(\n  model = \"Lasso no sd\",\n  lambda_index = 1:path_length,\n  classification_rate = lasso_cr_no_sd\n)\nmax(lasso_df_no_sd$classification_rate)\n</code></pre> We obtain a peak accuracy of , which is lower than the one obtained without standardising (). Standardising scales the data matrix, which is important in regression models as it allows for more direct comparison between the genes, and this is a demonstration of how it can lead to better predictive performance.</p>"},{"location":"6-chapter-3-solutions/#q2-apply-the-lasso-to-the-cancer-data","title":"Q2: apply the lasso to the cancer data.","text":"<p>First, we need to split the data as for the colitis data. There are only  observations this time, so we will split it 50/50. <pre><code>sewd(\"data\")\ndata = readRDS(\"cancer-data-c8.RDS\")\nX_cancer = data$X\ny_cancer = data$y\ngroups_cancer = data$groups\nrm(data)\nset.seed(100)\ntraining_ind &lt;- sample(1:nrow(X_cancer), 30) \ntrain_data_cancer &lt;- list(X = X_cancer[training_ind,], y = y_cancer[training_ind])\ntest_data_cancer &lt;- list(X = X_cancer[-training_ind,], y = y_cancer[-training_ind])\n</code></pre> We can proceed as before. <pre><code>lasso_model_cancer &lt;- glmnet(\n  x = train_data_cancer$X,\n  y = train_data_cancer$y,\n  family = \"binomial\",\n  lambda.min.ratio = min_frac,\n  maxit = num_iter,\n  standardize = TRUE,\n  intercept = TRUE\n) \n</code></pre> The fitted values are visualised using <pre><code>plot(lasso_model_cancer)\n</code></pre></p> <p></p> <p>There appears to be a trend towards negative coefficients, indicating that there are genes present which reduce the probability of cancer. Looking at how the variables enter the model: <pre><code>plot(apply(lasso_model_cancer$beta, 2, function(x) length(which(x!=0))), type=\"l\", xlab=\"Lambda index\", ylab = \"Number non-zero\")\n</code></pre></p> <p></p> <p>At the most saturated point, we have just below  genes in the model. Now, testing this model on new data.  <pre><code># calculate predictions\nlasso_preds &lt;- predict(lasso_model_cancer, test_data_cancer$X, type = \"class\")\n# compare to test data\nlasso_cr &lt;- apply(lasso_preds, 2, function(x) mean(x == test_data_cancer$y))\n\n# put classification scores into data frame\nlasso_df_cancer = data.frame(\n  model = \"Lasso\",\n  lambda_index = 1:path_length,\n  classification_rate = lasso_cr\n)\nplot(x = lasso_df_cancer$lambda_index, y = lasso_df_cancer$classification_rate, type=\"l\", xlab=\"Lambda index\", ylab = \"Classification accuracy\")\nabline(v = which.max(lasso_df_cancer$classification_rate), col = \"red\") # where the maximum is located\nmax(lasso_df_cancer$classification_rate)\nwhich.max(lasso_df_cancer$classification_rate)\napply(lasso_model_cancer$beta, 2, function(x) length(which(x!=0)))[which.max(lasso_df_cancer$classification_rate)]\n</code></pre></p> <p></p> <p>The predictive performance here is much worse than for the colitis dataset. This is to be expected. The development of cancer follows a more complex genetic landscape, making prediction very challenging. The particularly interesting aspect here is that the model does not actually improve the predictive performance by adding genes. The best performing model is the one with no variables present, showing that the lasso was not able to identify any signal in the dataset. </p>"},{"location":"6-chapter-3-solutions/#q3-optional-glmnet-has-the-elastic-net-model-apply-it-to-the-colitis-data","title":"Q3 (optional): <code>glmnet</code> has the elastic net model. Apply it to the colitis data.","text":"<p>We investigated the elastic net in Chapter 2. Here, we apply it to the colitis dataset to see if we can improve upon the lasso performance. <pre><code>alpha_seq = seq(from = 0, to = 1, length.out = 20)\nalpha_data = data.frame(alpha_val = alpha_seq, pred_score = rep(0,length(alpha_seq)))\nfor (alpha_id in 1:length(alpha_seq)){\nen_model &lt;- glmnet(\n  x = train_data$X,\n  y = train_data$y,\n  family = \"binomial\",\n  lambda.min.ratio = min_frac,\n  maxit = num_iter,\n  standardize = TRUE,\n  intercept = TRUE,\n  alpha = alpha_seq[alpha_id]\n) \nen_preds &lt;- predict(en_model, test_data$X, type = \"class\")\nen_cr &lt;- apply(en_preds, 2, function(x) mean(x == test_data$y))\nalpha_data$pred_score[alpha_id] = max(en_cr)}\nplot(alpha_data, type = \"b\", xlab = \"Alpha\", ylab = \"Classification accuracy\")\n</code></pre></p> <p></p> <p>We see that the predictive performance is not hugely sensitive to the choice of , although it is clear that the elastic net can improve over the lasso by over , if we choose  to be in the region of . For the lasso, we had a peak of  and for elastic net we have obtained .</p>"},{"location":"6-chapter-3-solutions/#q4-apply-the-group-lasso-to-the-cancer-data","title":"Q4: apply the group lasso to the cancer data.","text":"<p>As before: <pre><code>X_gl &lt;- t(t(train_data_cancer$X) - apply(train_data_cancer$X, 2, mean))\nX_gl &lt;- t(t(X_gl) / apply(X_gl, 2, sd))\nX_gl &lt;- cbind(1, X_gl)\ngroups_gl &lt;- c(NA, groups_cancer)\n\nlambda_max_group &lt;- lambdamax(X_gl, as.numeric(train_data_cancer$y), groups_gl, standardize = FALSE)\nlambdas_gl &lt;- exp(seq(\n  from = log(lambda_max_group),\n  to = log(lambda_max_group * min_frac),\n  length.out = path_length\n))\nglasso_model_cancer &lt;- grplasso(\n  x = X_gl,\n  y = as.numeric(train_data_cancer$y),\n  index = groups_gl,\n  lambda = lambdas_gl,\n  standardize = FALSE,\n  max.iter = num_iter\n)\n</code></pre></p> <p>We can see how many variables are entering the model as we decrease : <pre><code>plot(apply(glasso_model_cancer$coefficients, 2, function(x) length(which(x!=0))), type=\"l\", xlab=\"Lambda index\", ylab = \"Number non-zero\")\n</code></pre></p> <p></p> <p>At the most saturated point, we have about  genes in the model. Performing the prediction <pre><code>glasso_preds &lt;- predict(object = glasso_model_cancer, newdata = cbind(1,test_data_cancer$X), type = \"response\")\nglasso_preds = ifelse(glasso_preds &gt;= 0.5, 1, 0)\nglasso_cr &lt;- apply(glasso_preds, 2, function(x) mean(x == test_data_cancer$y))\n\n# put classification scores into data frame\nglasso_df_cancer = data.frame(\n  model = \"gLasso\",\n  lambda_index = 1:path_length,\n  classification_rate = glasso_cr\n)\nplot(x = glasso_df_cancer$lambda_index, y = glasso_df_cancer$classification_rate, type=\"l\", xlab=\"Lambda index\", ylab = \"Classification accuracy\")\nabline(v = which.max(glasso_df_cancer$classification_rate), col = \"red\") # where the maximum is located\nmax(glasso_df_cancer$classification_rate)\nlength(which(glasso_model_cancer$coefficients[,which.max(glasso_df_cancer$classification_rate)]!=0))\n</code></pre></p> <p></p> <p>The group lasso obtains a peak accuracy of  using  genes. In this case, it outperforms the lasso, showing that the grouping information is needed to extract the signal from the cancer genes.</p>"},{"location":"6-chapter-3-solutions/#q5-apply-sgl-to-the-cancer-data","title":"Q5: apply SGL to the cancer data.","text":"<p><pre><code>library(sgs)\nsgl_model_cancer = fit_sgs(\n  X = train_data_cancer$X,\n  y = train_data_cancer$y,\n  groups = groups_cancer,\n  type = \"logistic\",\n  path_length = path_length,\n  min_frac = min_frac,\n  alpha = alpha,\n  max_iter = num_iter,\n  screen = TRUE,\n  intercept = FALSE,\n  verbose = TRUE,\n  v_weights = rep(1, ncol(train_data_cancer$X)),\n  w_weights = rep(1, length(unique(groups_cancer)))\n)\n</code></pre> Performing the predictiton: <pre><code>sgl_preds = predict(sgl_model_cancer, x = test_data_cancer$X)\nsgl_cr &lt;- apply(sgl_preds$class, 2, function(x) mean(x == test_data_cancer$y))\n\n# put classification scores into data frame\nsgl_df_cancer = data.frame(\n  model = \"SGL\",\n  lambda_index = 1:path_length,\n  classification_rate = sgl_cr\n)\nmax(sgl_df_cancer$classification_rate)\nwhich.max(sgl_df_cancer$classification_rate)\n</code></pre> SGL obtains a peak accuracy of  at the index of , using no genes: <pre><code>length(sgl_model_cancer$selected_var[[which.max(sgl_df_cancer$classification_rate)]])\n</code></pre></p>"},{"location":"6-chapter-3-solutions/#q6-optional-apply-slope-to-the-cancer-data","title":"Q6 (optional): apply SLOPE to the cancer data.","text":"<p><pre><code>slope_model_cancer = SLOPE(\n  x = train_data_cancer$X,\n  y = train_data_cancer$y,\n  q = 1e-4,\n  family = \"binomial\",\n  intercept = TRUE,\n  scale = \"l2\",\n  center = TRUE,\n  alpha = \"path\",\n  lambda = \"bh\",\n  alpha_min_ratio = min_frac,\n  max_passes = 10000,\n  path_length = path_length,\n  tol_dev_ratio = 1000000,\n  max_variables = 10000,\n  tol_dev_change = 0\n)\n\nslope_preds = predict(slope_model_cancer, x = test_data_cancer$X, type = \"response\") \nslope_preds = ifelse(slope_preds &gt;= 0.5, 1, 0)\nslope_cr &lt;- apply(slope_preds, 2, function(x) mean(x == test_data_cancer$y))\n\n# put classification scores into data frame\nslope_df_cancer = data.frame(\n  model = \"SLOPE\",\n  lambda_index = 1:path_length,\n  classification_rate = slope_cr\n)\nmax(slope_df_cancer$classification_rate)\nsum(slope_model_cancer$nonzeros[,,which.max(slope_df_cancer$classification_rate)])\n</code></pre> SLOPE is found to have a peak classification rate of  using  genes. So, the same accuracy as the lasso, but actually using genes. However, the genes were not found to be informative.</p>"},{"location":"6-chapter-3-solutions/#q7-optional-apply-gslope-to-the-cancer-data","title":"Q7 (optional): apply gSLOPE to the cancer data.","text":"<p><pre><code>gslope_model_cancer = fit_gslope(\n  X = train_data_cancer$X,\n  y = train_data_cancer$y,\n  groups = groups_cancer,\n  type = \"logistic\",\n  path_length = path_length,\n  min_frac = min_frac,\n  gFDR = 1e-4, \n  max_iter = num_iter,\n  screen = TRUE,\n  intercept = FALSE,\n  verbose = TRUE\n)\n\ngslope_preds = predict(gslope_model_cancer, x = test_data_cancer$X)\ngslope_cr &lt;- apply(gslope_preds$class, 2, function(x) mean(x == test_data_cancer$y))\n\n# put classification scores into data frame\ngslope_df_cancer = data.frame(\n  model = \"gSLOPE\",\n  lambda_index = 1:path_length,\n  classification_rate = gslope_cr\n)\nmax(gslope_df_cancer$classification_rate)\nlength(gslope_model_cancer$selected_var[[which.max(gslope_df_cancer$classification_rate)]])\n</code></pre> gSLOPE obtains a peak accuracy of  using  genes, so it is picking up a lot of noise. </p>"},{"location":"6-chapter-3-solutions/#q8-optional-can-you-achieve-a-higher-predictive-accuracy-with-sgs","title":"Q8 (optional): can you achieve a higher predictive accuracy with SGS?","text":"<p>SGS has a few hyperparameters to play around with. Feel free to try changing the different hyperparameters and seeing what the result is. Here, I will alter two to give you an insight into how they can change the model performance.</p> <p>We first alter  to be . <pre><code>sgs_model_2 = fit_sgs(\n  X = train_data$X,\n  y = train_data$y,\n  groups = groups,\n  type = \"logistic\",\n  path_length = path_length,\n  min_frac = min_frac,\n  gFDR = 1e-4,\n  vFDR = 1e-4, \n  alpha = 0.5,\n  max_iter = num_iter,\n  screen = TRUE,\n  intercept = FALSE,\n  verbose = TRUE,\n  pen_method = 3  \n)\n\nsgs_preds = predict(sgs_model_2, x = test_data$X)\nsgs_cr &lt;- apply(sgs_preds$class, 2, function(x) mean(x == test_data$y))\nmax(sgs_cr)\nlength(sgs_model_2$selected_var[[which.max(sgs_cr)]])\n</code></pre> This model obtains a peak accuracy of  using  genes. So, changing  has lead to worse performance. This was to be expected, as by moving  away from  we also moved the model closer to a gSLOPE model, which performs worse than SLOPE for the colitis data.</p> <p>Next, we alter the penalty sequences through the choice of the FDR hyperparameters (<code>gFDR</code> and <code>vFDR</code>). I will set these to be significantly smaller, in the hope of inducing more sparsity in the model. The aim here is to try to remove as much noise from the model as possible (without removing all of the signal).  <pre><code>sgs_model_3 = fit_sgs(\n  X = train_data$X,\n  y = train_data$y,\n  groups = groups,\n  type = \"logistic\",\n  path_length = path_length,\n  min_frac = min_frac,\n  gFDR = 1e-10,\n  vFDR = 1e-10, \n  alpha = alpha,\n  max_iter = num_iter,\n  screen = TRUE,\n  intercept = FALSE,\n  verbose = TRUE,\n  pen_method = 3  \n)\n\nsgs_preds = predict(sgs_model_3, x = test_data$X)\nsgs_cr &lt;- apply(sgs_preds$class, 2, function(x) mean(x == test_data$y))\nmax(sgs_cr)\nlength(sgs_model_3$selected_var[[which.max(sgs_cr)]])\n</code></pre> This model obtains a peak accuracy of  using only  genes. Here, we have an example of how the sparse-group models can be powerful when implemented correctly. This is the highest predictive accuracy we have obtained so far.</p>"},{"location":"6-chapter-3-solutions/#q9-optional-apply-sgs-to-the-cancer-data","title":"Q9 (optional): apply SGS to the cancer data.","text":"<p><pre><code>sgs_model_cancer = fit_sgs(\n  X = train_data_cancer$X,\n  y = train_data_cancer$y,\n  groups = groups_cancer,\n  type = \"logistic\",\n  path_length = path_length,\n  min_frac = min_frac,\n  gFDR = 1e-4,\n  vFDR = 1e-4, \n  alpha = alpha,\n  max_iter = num_iter,\n  screen = TRUE,\n  intercept = FALSE,\n  verbose = TRUE,\n  pen_method = 3\n)\n\nsgs_preds = predict(sgs_model_cancer, x = test_data_cancer$X)\nsgs_cr &lt;- apply(sgs_preds$class, 2, function(x) mean(x == test_data_cancer$y))\n\n# put classification scores into data frame\nsgs_df_cancer = data.frame(\n  model = \"SGS\",\n  lambda_index = 1:path_length,\n  classification_rate = sgs_cr\n)\nmax(sgs_df_cancer$classification_rate)\n\nlength(sgs_model$selected_var[[which.max(sgs_df$classification_rate)]])\n</code></pre> SGS obtains a peak accuracy of  using  genes. However, we could try changing the sequence (as we did for Q8), to see if we get better performance <pre><code>sgs_model_cancer_2 = fit_sgs(\n  X = train_data_cancer$X,\n  y = train_data_cancer$y,\n  groups = groups_cancer,\n  type = \"logistic\",\n  path_length = path_length,\n  min_frac = min_frac,\n  gFDR = 1e-10,\n  vFDR = 1e-10, \n  alpha = alpha,\n  max_iter = num_iter,\n  screen = TRUE,\n  intercept = FALSE,\n  verbose = TRUE,\n  pen_method = 3\n)\n\nsgs_preds = predict(sgs_model_cancer_2, x = test_data_cancer$X)\nsgs_cr &lt;- apply(sgs_preds$class, 2, function(x) mean(x == test_data_cancer$y))\nmax(sgs_cr)\nlength(sgs_model$selected_var[[which.max(sgs_cr)]])\n</code></pre> In this case, this did not help, as we stay at the same accuracy.</p> <p>We end the section by comparing all of the models on the cancer dataset.</p>"},{"location":"6-chapter-3-solutions/#number-of-non-zero-coefficients","title":"Number of non-zero coefficients","text":"<pre><code>plot(apply(lasso_model_cancer$beta, 2, function(x) length(which(x!=0))), type=\"l\", xlab=\"Lambda index\", ylab = \"Number non-zero\",ylim=c(0,55))\nlines(apply(glasso_model_cancer$coefficients, 2, function(x) length(which(x!=0))), type=\"l\", col = \"red\")\nlines(unlist(lapply(sgl_model_cancer$selected_var,length)), type = \"l\", col = \"brown\")\nlines(apply(slope_model_cancer$nonzeros,3,sum), type = \"l\", col = \"blue\")\nlines(unlist(lapply(gslope_model_cancer$selected_var,length)), type = \"l\", col = \"green\")\nlines(unlist(lapply(sgs_model_cancer$selected_var,length)), type = \"l\", col = \"purple\")\nlegend(\"bottomright\", legend = c(\"Lasso\", \"gLasso\", \"SGL\", \"SLOPE\", \"gSLOPE\", \"SGS\"),\n       col = c(\"black\", \"red\", \"brown\", \"blue\", \"green\", \"purple\"), lty = 1)\n</code></pre>"},{"location":"6-chapter-3-solutions/#prediction-accuracies","title":"Prediction accuracies","text":"<pre><code>plot(lasso_df_cancer$classification_rate, type=\"l\", xlab=\"Lambda index\", ylab = \"Classification accuracy\", ylim=c(0.3,0.6))\nlines(glasso_df_cancer$classification_rate, type=\"l\", col = \"red\")\nlines(sgl_df_cancer$classification_rate, type = \"l\", col = \"brown\")\nlines(slope_df_cancer$classification_rate, type = \"l\", col = \"blue\")\nlines(gslope_df_cancer$classification_rate, type = \"l\", col = \"green\")\nlines(sgs_df_cancer$classification_rate, type = \"l\", col = \"purple\")\nlegend(\"bottomright\", legend = c(\"Lasso\", \"gLasso\", \"SGL\", \"SLOPE\", \"gSLOPE\", \"SGS\"),\n       col = c(\"black\", \"red\", \"brown\", \"blue\", \"green\", \"purple\"), lty = 1)\n</code></pre>"},{"location":"6-chapter-3-solutions/#prediction-accuracies-on-cancer-dataset","title":"Prediction accuracies on cancer dataset","text":"Model Classification accuracy (%) Genes used Lasso 56.7 0 gLasso 60.0 15 SGL 56.7 0 SLOPE 56.7 43 gSLOPE 33.3 417 SGS 56.7 23"}]}